{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Results\"\n",
    "section-divs: false\n",
    "toc: false\n",
    "site-path: \".\"\n",
    "format:\n",
    "    html:\n",
    "        code-fold: true\n",
    "execute:\n",
    "    echo: false\n",
    "    warning: false\n",
    "    output: asis\n",
    "\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Physical models of rigid bodies are used for sound synthesis in applications from virtual environments to music production.\n",
    "Traditional methods such as modal synthesis often rely on computationally expensive numerical solvers, while recent deep learning approaches are limited by post-processing of their results.\n",
    "In this work we present a novel end-to-end framework for training a deep neural network to generate modal resonators for a given 2D shape and material, using a bank of differentiable IIR filters.\n",
    "We demonstrate our method on a dataset of synthetic objects, but train our model using an audio-domain objective, paving the way for physically-informed synthesisers to be learned directly from recordings of real-world objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from nbdev import qmd\n",
    "\n",
    "from neuralresonator.modal import Material, MaterialRanges, System\n",
    "from neuralresonator.shape import generate_convex_mesh, polygon2mask\n",
    "from neuralresonator.training import MultiShapeMultiMaterialLitModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "\n",
    "def im(filename, width, **kw): return qmd.img(filename, width=f\"{width}%\", **kw)\n",
    "def audio(filename, classes=None, width=None, **kw):\n",
    "    style = {}\n",
    "    style[\"width\"] = \"100px\"\n",
    "    style[\"height\"] = \"50px\"\n",
    "    style[\"max-width\"] = \"100px\"\n",
    "    classes = [\"audio-container\"]\n",
    "    return qmd.meta(f'![]({filename})', classes=classes, style=style, **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "# Paths\n",
    "model_ckpt = Path(\"../data/ethereal_dust-317-2.ckpt\")\n",
    "\n",
    "# Figures path\n",
    "figures_path = Path('assets/results/figures')\n",
    "audio_path = Path('assets/results/audio')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the original audio and the generated audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||||\n",
      "|--|--|--|--|--|--|\n",
      "|Random shape/material|![](assets/results/figures/occupancy_0_15.png){width=\"200%\"}|![](assets/results/figures/occupancy_12_15.png){width=\"200%\"}|![](assets/results/figures/occupancy_24_15.png){width=\"200%\"}|![](assets/results/figures/occupancy_36_15.png){width=\"200%\"}|![](assets/results/figures/occupancy_48_15.png){width=\"200%\"}|\n",
      "|Spectrogram original|![](assets/results/figures/gt_specgram_0_15.png){width=\"200%\"}|![](assets/results/figures/gt_specgram_12_15.png){width=\"200%\"}|![](assets/results/figures/gt_specgram_24_15.png){width=\"200%\"}|![](assets/results/figures/gt_specgram_36_15.png){width=\"200%\"}|![](assets/results/figures/gt_specgram_48_15.png){width=\"200%\"}|\n",
      "|Spectrogram predicted|![](assets/results/figures/pred_specgram_0_15.png){width=\"200%\"}|![](assets/results/figures/pred_specgram_12_15.png){width=\"200%\"}|![](assets/results/figures/pred_specgram_24_15.png){width=\"200%\"}|![](assets/results/figures/pred_specgram_36_15.png){width=\"200%\"}|![](assets/results/figures/pred_specgram_48_15.png){width=\"200%\"}|\n",
      "|Audio original|![](assets/results/audio/gt_0_15.wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|![](assets/results/audio/gt_12_15.wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|![](assets/results/audio/gt_24_15.wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|![](assets/results/audio/gt_36_15.wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|![](assets/results/audio/gt_48_15.wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n",
      "|Audio predicted|![](assets/results/audio/pred_0_15.wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|![](assets/results/audio/pred_12_15.wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|![](assets/results/audio/pred_24_15.wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|![](assets/results/audio/pred_36_15.wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|![](assets/results/audio/pred_48_15.wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n"
     ]
    }
   ],
   "source": [
    "# gather 5 results for the table\n",
    "occ_maps = sorted(list(figures_path.glob('occupancy_*')))[:5]\n",
    "\n",
    "gt_figures = sorted(list(figures_path.glob('gt_specgram_*')))[:5]\n",
    "pred_figures = sorted(list(figures_path.glob('pred_specgram_*')))[:5]\n",
    "gt_audio = sorted(list(audio_path.glob('gt_*')))[:5]\n",
    "pred_audio = sorted(list(audio_path.glob('pred_*')))[:5]\n",
    "\n",
    "print(qmd.tbl_row(['','','']))\n",
    "print(qmd.tbl_sep([2,2,2,2,2,2]))\n",
    "print(qmd.tbl_row([\"Random shape/material\"] + [im(i, 200) for i in occ_maps]))\n",
    "print(qmd.tbl_row([\"Spectrogram original\"] + [im(i, 200) for i in gt_figures]))\n",
    "print(qmd.tbl_row([\"Spectrogram predicted\"] + [im(i, 200) for i in pred_figures]))\n",
    "print(qmd.tbl_row([\"Audio original\"] + [audio(i, 200) for i in gt_audio]))\n",
    "print(qmd.tbl_row([\"Audio predicted\"] + [audio(i, 200) for i in pred_audio]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diaz/anaconda3/envs/neuralresonator/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:197: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "  rank_zero_warn(\n",
      "/home/diaz/anaconda3/envs/neuralresonator/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:197: UserWarning: Attribute 'criterion' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['criterion'])`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "\n",
    "model = MultiShapeMultiMaterialLitModule.load_from_checkpoint(\n",
    "    checkpoint_path=model_ckpt,\n",
    "    map_location=\"cpu\"\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "Material.set_default_ranges(\n",
    "    MaterialRanges(\n",
    "        rho=(500, 15000),\n",
    "        E=(8.e+9, 5.e+10),\n",
    "        nu=(0.1, 0.4),\n",
    "        alpha=(1, 10),\n",
    "        beta=(3.e-7, 2.e-6),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 3407\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "\n",
    "# create a shape and rasterize it\n",
    "pl.seed_everything(3407)\n",
    "\n",
    "n_materials = 5\n",
    "n_shapes = 1\n",
    "n_vertices = 10\n",
    "n_refinements = 3\n",
    "\n",
    "resolution = (64, 64)\n",
    "mesh, points = generate_convex_mesh(n_vertices, n_refinements)\n",
    "mask = polygon2mask(resolution, ((points + 1) / 2) * resolution[0]).T[None, ...]\n",
    "mask = torch.from_numpy(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAIq0lEQVR4nO3dwYpb5xnH4fdzh/GYeBFoDKWBbFNaQseb5ApKaRcNWfQW0ptIwBBfROpb6Cbppji9AheKB0wgWYeYghPwwsWKOvjrQpO/TTxCGulIOiM/zybgjE8Oxpkf75xX32m9914AUFVXdn0DAIyHKAAQogBAiAIAIQoAhCgAEKIAQIgCAHGw7Bf+7sqfN3kfAGzYP5/9beHXmBQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgDjY9Q0Ar6a7D092fQuj9PtfHu/0v29SACBEAYAQBQBCFAAID5phhzxs5afm/Z3Y1gNokwIAIQoAhCgAEKIAQIgCAGH7iK2xaQPjZ1IAIEQBgBAFAEIUAAhRACBsH22R7Rtg7EwKAIQoABCiAECIAgAhCgDEVrePbN8ArOa875+beBubSQGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAILb6kh0AVrOJF+qcx6QAQIgCACEKAIQoABCiAEDYPoJ1nEyqff6k6v6k6utp1Q+96mqrevuw6uZR9fevVx0f7fouYWmiAKu497Tax4+qPZhWP6iq06r247972qvfm1T9e1JXPn1c/Z3D6p/cqHrv2g5vGJbjx0dwEae92q3vqn3wbdWX06qqai8G4Uw7+/WqqvpyWu2Db6vd+q7qtG/zbuHCTAqwrNNe7S//qfrHf6v1qlry+3t7Nvtnv/O42jf/q/7XX1Qd/DQjMA4mBVhSu/398yCs8vt7zX7/7e8HvS8YkijAMu49rbrzeOUg/Kj1qrrzuOpfT4e4KxicHx/BEtrHj2YPCoZ4JNCq2kePqn/x1gAXY99s64yjeUwKsMjJpNqDaZ4NrKs9q2oPplUnk2EuCAMSBVigff5ktnY6oH5Q1f7+ZNiLwgBEARa5P6k6XfxlF3J6dl0YGVGARb6evvQ5hHW1qqqvpgNfFdYnCrDIDxv6wNmmrgtrEAVY5OqGPmi2qevCGkQBFnn7cJBN1Bf1qqpfHQ58VVifKMAiN4+G/0TPwdl1YWREARbo719/frjdQNrp7LowNqIAixwfzY6/Huj/ln6lqr9zWPVbkwLjs9VjLuZ9fPvuw5Nt3gZcWP/kxuy47EEuVtVv3xjmWjAwkwIs471rVR++Xn3NhaHequrD16ve9cIdxkkUYEn9o59X/eG1lcPQW1X98bXZdWCkRAGWddBmL8g5mxiWfcbQf/Z8QuifesEO4yYKcBEHrfqtN6p/9mbVb2afM+gHL5+o3c9+vaqqfn1Y/bM3q996QxAYPe9TgFW8e636F29VP5nMTju9P6n+1XR2dMXVNvtg2s2jevan61XHtoy4PEQB1nF8VN03fVaw65fpzOPHRwCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAEA4+whgg8Z6xtE8JgUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIx1wADOSyHWlxHpMCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAEB4yQ7ABe3Dy3TmMSkAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIA4ewjgDn2+YyjeUwKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIzi7KN554vcfXiy1fsAXk2v4hlH85gUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEZxzAXANjjOYjGTAgAhCgCEKAAQogBAiAIAMertIy/fAdgukwIAIQoAhCgAEKIAQIgCADHq7aN5zttKspEEsD6TAgAhCgCEKAAQogBAXMoHzedxJAbwIi/UWY1JAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAIi9OftoHmciwX5zxtGwTAoAhCgAEKIAQIgCACEKAMTebx/NYysJ4GUmBQBCFAAIUQAgRAGAEAUA4pXdPprHVhKMkzOOtsOkAECIAgAhCgCEKAAQHjQv6byHXB4+A/vGpABAiAIAIQoAhCgAEKIAQNg+WoMjMYB9Y1IAIEQBgBAFAEIUAAhRACBsH22ArSRYnZfp7JZJAYAQBQBCFAAIUQAgRAGAsH20RRfZqrCpBOyCSQGAEAUAQhQACFEAIDxoHqmhPurvgTVj5TiLcTIpABCiAECIAgAhCgCEKAAQto/2nKM1gIswKQAQogBAiAIAIQoAhCgAELaPCOctsSnOObo8TAoAhCgAEKIAQIgCACEKAITtIwa3yU0Tm02wWSYFAEIUAAhRACBEAYAQBQDC9hGXypjO0LEJxT4yKQAQogBAiAIAIQoAhAfNsKKxPPQe0wPvsfyZsDqTAgAhCgCEKAAQogBAiAIAYfsILjkbPwzJpABAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAEC03nvf9U0AMA4mBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAOL/gqELuKe0snwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | hide\n",
    "\n",
    "nodes = mesh.interior_nodes()\n",
    "node = np.random.choice(nodes)\n",
    "coords = torch.from_numpy((mesh.p.T[node] + 1) / 2).float()\n",
    "\n",
    "plt.axis('off')\n",
    "fig = plt.imshow(mask.permute(1, 2, 0))\n",
    "fig = plt.scatter(\n",
    "    x=(coords[0] * mask.shape[-1]),\n",
    "    y=(coords[1] * mask.shape[-1]),\n",
    "    c='r',\n",
    "    s=240,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::: {.column text-align=\"center\"}\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials_rho = [Material(rho, 2.9e+10, 0.25, 5.5, 1.15e-6) for rho in np.linspace(500, 15000, 10)]\n",
    "materials_nu = [Material(7750, 2.9e+10, nu, 5.5, 1.15e-6) for nu in np.linspace(0.1, 0.49, 10)]\n",
    "materials_list = [materials_rho, materials_nu]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sr = 32000\n",
    "time_in_seconds = 0.2\n",
    "impulse = torch.zeros(int(time_in_seconds * sr))\n",
    "impulse[0] = 1.0\n",
    "k=32\n",
    "material_interp_dir = Path(\"assets/results/audio/material_interp\")\n",
    "material_interp_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "from neuralresonator.dsp import apply_filter\n",
    "with torch.no_grad():\n",
    "    \n",
    "    features = model.encoder(mask[None, ...].repeat(1, 3, 1, 1).float())\n",
    "\n",
    "    for mat_list in materials_list:\n",
    "        for material in mat_list:\n",
    "\n",
    "            # create a system\n",
    "            system = System(\n",
    "                material=material,\n",
    "                mesh=mesh,\n",
    "                k=k,\n",
    "                force_cache=True\n",
    "            )\n",
    "\n",
    "            gt_audio = system.render(time_in_seconds, sr, impulse_node_idx=node)\n",
    "\n",
    "            material_params = torch.tensor([*(material.scaled())]).float()\n",
    "\n",
    "            # Predict biquad coefficients\n",
    "            ba = model.forward(torch.cat([features, coords[None, ...], material_params[None, ...]], dim=-1))\n",
    "            b = ba[..., :3]\n",
    "            a = ba[..., 3:]\n",
    "\n",
    "            pred_signal = apply_filter(\n",
    "                impulse.cpu().numpy(),\n",
    "                b=b[0].cpu().numpy(),\n",
    "                a=a[0].cpu().numpy(),\n",
    "            )\n",
    "            \n",
    "            sf.write(f\"assets/results/audio/material_interp/gt_{material}.wav\", gt_audio, sr)\n",
    "            sf.write(f\"assets/results/audio/material_interp/pred_{material}.wav\", pred_signal, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Density|Original|Predicted|\n",
      "|--|--|--|\n",
      "|$\\rho=500.00$|![](assets/results/audio/material_interp/gt_Material(rho=500.0, E=29000000000.0, nu=0.25, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|![](assets/results/audio/material_interp/pred_Material(rho=500.0, E=29000000000.0, nu=0.25, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n",
      "|$\\rho=2111.11$|![](assets/results/audio/material_interp/gt_Material(rho=2111.1111111111113, E=29000000000.0, nu=0.25, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|![](assets/results/audio/material_interp/pred_Material(rho=2111.1111111111113, E=29000000000.0, nu=0.25, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n",
      "|$\\rho=3722.22$|![](assets/results/audio/material_interp/gt_Material(rho=3722.222222222222, E=29000000000.0, nu=0.25, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|![](assets/results/audio/material_interp/pred_Material(rho=3722.222222222222, E=29000000000.0, nu=0.25, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n",
      "|$\\rho=5333.33$|![](assets/results/audio/material_interp/gt_Material(rho=5333.333333333333, E=29000000000.0, nu=0.25, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|![](assets/results/audio/material_interp/pred_Material(rho=5333.333333333333, E=29000000000.0, nu=0.25, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n",
      "|$\\rho=6944.44$|![](assets/results/audio/material_interp/gt_Material(rho=6944.444444444444, E=29000000000.0, nu=0.25, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|![](assets/results/audio/material_interp/pred_Material(rho=6944.444444444444, E=29000000000.0, nu=0.25, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n",
      "|$\\rho=8555.56$|![](assets/results/audio/material_interp/gt_Material(rho=8555.555555555555, E=29000000000.0, nu=0.25, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|![](assets/results/audio/material_interp/pred_Material(rho=8555.555555555555, E=29000000000.0, nu=0.25, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n",
      "|$\\rho=10166.67$|![](assets/results/audio/material_interp/gt_Material(rho=10166.666666666666, E=29000000000.0, nu=0.25, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|![](assets/results/audio/material_interp/pred_Material(rho=10166.666666666666, E=29000000000.0, nu=0.25, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n",
      "|$\\rho=11777.78$|![](assets/results/audio/material_interp/gt_Material(rho=11777.777777777777, E=29000000000.0, nu=0.25, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|![](assets/results/audio/material_interp/pred_Material(rho=11777.777777777777, E=29000000000.0, nu=0.25, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n",
      "|$\\rho=13388.89$|![](assets/results/audio/material_interp/gt_Material(rho=13388.888888888889, E=29000000000.0, nu=0.25, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|![](assets/results/audio/material_interp/pred_Material(rho=13388.888888888889, E=29000000000.0, nu=0.25, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n",
      "|$\\rho=15000.00$|![](assets/results/audio/material_interp/gt_Material(rho=15000.0, E=29000000000.0, nu=0.25, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|![](assets/results/audio/material_interp/pred_Material(rho=15000.0, E=29000000000.0, nu=0.25, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n"
     ]
    }
   ],
   "source": [
    "# open the audio files\n",
    "print(qmd.tbl_row(['Density', 'Original', 'Predicted' ]))\n",
    "print(qmd.tbl_sep([2,2,2]))\n",
    "for material in materials_rho:\n",
    "    print(\n",
    "        qmd.tbl_row(\n",
    "            [f\"$\\\\rho={material.rho:.2f}$\"] +\n",
    "            [audio(f\"assets/results/audio/material_interp/gt_{material}.wav\", 100)] + \n",
    "            [audio(f\"assets/results/audio/material_interp/pred_{material}.wav\", 100)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Poisson's ratio|Original|Predicted|\n",
      "|--|--|--|\n",
      "|$\\nu=0.10$|![](assets/results/audio/material_interp/gt_Material(rho=7750, E=29000000000.0, nu=0.1, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|![](assets/results/audio/material_interp/pred_Material(rho=7750, E=29000000000.0, nu=0.1, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n",
      "|$\\nu=0.14$|![](assets/results/audio/material_interp/gt_Material(rho=7750, E=29000000000.0, nu=0.14333333333333334, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|![](assets/results/audio/material_interp/pred_Material(rho=7750, E=29000000000.0, nu=0.14333333333333334, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n",
      "|$\\nu=0.19$|![](assets/results/audio/material_interp/gt_Material(rho=7750, E=29000000000.0, nu=0.18666666666666668, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|![](assets/results/audio/material_interp/pred_Material(rho=7750, E=29000000000.0, nu=0.18666666666666668, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n",
      "|$\\nu=0.23$|![](assets/results/audio/material_interp/gt_Material(rho=7750, E=29000000000.0, nu=0.23, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|![](assets/results/audio/material_interp/pred_Material(rho=7750, E=29000000000.0, nu=0.23, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n",
      "|$\\nu=0.27$|![](assets/results/audio/material_interp/gt_Material(rho=7750, E=29000000000.0, nu=0.2733333333333333, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|![](assets/results/audio/material_interp/pred_Material(rho=7750, E=29000000000.0, nu=0.2733333333333333, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n",
      "|$\\nu=0.32$|![](assets/results/audio/material_interp/gt_Material(rho=7750, E=29000000000.0, nu=0.31666666666666665, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|![](assets/results/audio/material_interp/pred_Material(rho=7750, E=29000000000.0, nu=0.31666666666666665, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n",
      "|$\\nu=0.36$|![](assets/results/audio/material_interp/gt_Material(rho=7750, E=29000000000.0, nu=0.36, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|![](assets/results/audio/material_interp/pred_Material(rho=7750, E=29000000000.0, nu=0.36, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n",
      "|$\\nu=0.40$|![](assets/results/audio/material_interp/gt_Material(rho=7750, E=29000000000.0, nu=0.4033333333333333, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|![](assets/results/audio/material_interp/pred_Material(rho=7750, E=29000000000.0, nu=0.4033333333333333, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n",
      "|$\\nu=0.45$|![](assets/results/audio/material_interp/gt_Material(rho=7750, E=29000000000.0, nu=0.44666666666666666, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|![](assets/results/audio/material_interp/pred_Material(rho=7750, E=29000000000.0, nu=0.44666666666666666, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n",
      "|$\\nu=0.49$|![](assets/results/audio/material_interp/gt_Material(rho=7750, E=29000000000.0, nu=0.49, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|![](assets/results/audio/material_interp/pred_Material(rho=7750, E=29000000000.0, nu=0.49, alpha=5.5, beta=1.15e-06).wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n"
     ]
    }
   ],
   "source": [
    "print(qmd.tbl_row([\"Poisson's ratio\", 'Original', 'Predicted' ]))\n",
    "print(qmd.tbl_sep([2,2,2]))\n",
    "for material in materials_nu:\n",
    "    print(\n",
    "        qmd.tbl_row(\n",
    "            [f\"$\\\\nu={material.nu:.2f}$\"] +\n",
    "            [audio(f\"assets/results/audio/material_interp/gt_{material}.wav\", 100)] + \n",
    "            [audio(f\"assets/results/audio/material_interp/pred_{material}.wav\", 100)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate between two shapes, and see how the audio changes.\n",
    "> In this case, we do not have discretized positions for the coordinates in the shapes (as FEM requires discretization). Because our network acts as a neural field we can obtain sound for continuous coordinate values. We can interpolate between the two shapes and see how the audio changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "import scipy.optimize\n",
    "mesh0, points0 = generate_convex_mesh(n_points=5)\n",
    "mesh1, points1 = generate_convex_mesh(n_points=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a cost matrix with the distance between each point for each shape\n",
    "cost_matrix = scipy.spatial.distance.cdist(points0, points1)\n",
    "\n",
    "# find the optimal mapping between the two shapes\n",
    "row_ind, col_ind = scipy.optimize.linear_sum_assignment(cost_matrix)\n",
    "\n",
    "# interpolate 10 shapes between the two shapes\n",
    "n_shapes = 10\n",
    "rasterized_shapes = []\n",
    "for i in range(n_shapes):\n",
    "    alpha = i / n_shapes\n",
    "    points = (1 - alpha) * points0[row_ind] + alpha * points1[col_ind]\n",
    "    rasterized_shapes += [polygon2mask(resolution, ((points + 1) / 2) * resolution[0]).T[None, ...]]\n",
    "    \n",
    "# fix a coordinate in the shape\n",
    "coords = torch.tensor([0.6, 0.5]).float()\n",
    "\n",
    "# plot the interpolated shapes\n",
    "fig, axs = plt.subplots(1, 1)\n",
    "\n",
    "interpolation_path = Path(\"assets/results/interpolation\")\n",
    "interpolation_path.mkdir(exist_ok=True, parents=True)\n",
    "for i in range(n_shapes):\n",
    "    axs.imshow(rasterized_shapes[i].squeeze())\n",
    "    axs.scatter(\n",
    "        x=(coords[0] * mask.shape[-1]),\n",
    "        y=(coords[1] * mask.shape[-1]),\n",
    "        c='r',\n",
    "        s=240,\n",
    "    )\n",
    "    axs.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(interpolation_path / f\"interpolation_{i}.png\", bbox_inches='tight', pad_inches = 0)\n",
    "\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sound for each shape\n",
    "\n",
    "sr = 32000\n",
    "time_in_seconds = 0.2\n",
    "impulse = torch.zeros(int(time_in_seconds * 32000))\n",
    "impulse[0] = 1.0\n",
    "\n",
    "with torch.no_grad():\n",
    "        for idx, mask in enumerate(rasterized_shapes):\n",
    "\n",
    "            # convert to tensor\n",
    "            mask = torch.from_numpy(mask)\n",
    "            features = model.encoder(mask[None, ...].repeat(1, 3, 1, 1).float())\n",
    "\n",
    "            # Predict biquad coefficients\n",
    "            ba = model.forward(torch.cat([features, coords[None, ...], material_params[None, ...]], dim=-1))\n",
    "            b = ba[..., :3]\n",
    "            a = ba[..., 3:]\n",
    "\n",
    "            pred_signal = apply_filter(\n",
    "                impulse.cpu().numpy(),\n",
    "                b=b[0].cpu().numpy(),\n",
    "                a=a[0].cpu().numpy(),\n",
    "            )\n",
    "\n",
    "            sf.write(interpolation_path / f\"morph_{idx}.wav\", pred_signal, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Shape|Predicted sound|\n",
      "|--|--|\n",
      "|![](assets/results/interpolation/interpolation_0.png){width=\"50%\"}|![](assets/results/interpolation/morph_0.wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n",
      "|![](assets/results/interpolation/interpolation_1.png){width=\"50%\"}|![](assets/results/interpolation/morph_1.wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n",
      "|![](assets/results/interpolation/interpolation_2.png){width=\"50%\"}|![](assets/results/interpolation/morph_2.wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n",
      "|![](assets/results/interpolation/interpolation_3.png){width=\"50%\"}|![](assets/results/interpolation/morph_3.wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n",
      "|![](assets/results/interpolation/interpolation_4.png){width=\"50%\"}|![](assets/results/interpolation/morph_4.wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n",
      "|![](assets/results/interpolation/interpolation_5.png){width=\"50%\"}|![](assets/results/interpolation/morph_5.wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n",
      "|![](assets/results/interpolation/interpolation_6.png){width=\"50%\"}|![](assets/results/interpolation/morph_6.wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n",
      "|![](assets/results/interpolation/interpolation_7.png){width=\"50%\"}|![](assets/results/interpolation/morph_7.wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n",
      "|![](assets/results/interpolation/interpolation_8.png){width=\"50%\"}|![](assets/results/interpolation/morph_8.wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n",
      "|![](assets/results/interpolation/interpolation_9.png){width=\"50%\"}|![](assets/results/interpolation/morph_9.wav){.audio-container style=\"width: 100px; height: 50px; max-width: 100px\"}|\n"
     ]
    }
   ],
   "source": [
    "print(qmd.tbl_row(['Shape','Predicted sound']))\n",
    "print(qmd.tbl_sep([2,2]))\n",
    "for idx, occupancy_map in enumerate(rasterized_shapes):\n",
    "\n",
    "    print(\n",
    "        qmd.tbl_row(\n",
    "            [im(interpolation_path / f\"interpolation_{idx}.png\", 50)] + \n",
    "            [audio(interpolation_path / f\"morph_{idx}.wav\", 100)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
