{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "> Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List\n",
    "import math\n",
    "from neuralresonator.utilities import to_zpk\n",
    "from neuralresonator.dsp import (constrain_complex_pole_or_zero,\n",
    "                                 pole_or_zero_to_iir_coeff)\n",
    "from torchinfo import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class FCBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A fully connected block with an activation function and optional layer normalization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,  # The size of the input\n",
    "        output_size: int,  # The size of the output\n",
    "        activation: nn.Module = nn.LeakyReLU(\n",
    "            0.2, inplace=True\n",
    "        ),  # The activation function\n",
    "        layer_norm: bool = False,  # Whether to use layer normalization\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation = activation\n",
    "\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        self.ln = nn.LayerNorm(output_size) if layer_norm else nn.Identity()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        return self.activation(self.ln(self.linear(x)))\n",
    "\n",
    "\n",
    "class FC(nn.Module):\n",
    "    \"\"\"\n",
    "    A fully connected network with an activation function and optional layer normalization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,  # The size of the input\n",
    "        output_size: int,  # The size of the output\n",
    "        hidden_sizes: List[int],  # The sizes of the hidden layers\n",
    "        activation: nn.Module = nn.LeakyReLU(\n",
    "            0.2, inplace=True\n",
    "        ),  # The activation function\n",
    "        layer_norm: bool = False,  # Whether to use layer normalization\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.activation = activation\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # First layer\n",
    "        layers.append(\n",
    "            FCBlock(\n",
    "                input_size,\n",
    "                hidden_sizes[0],\n",
    "                activation=activation,\n",
    "                layer_norm=layer_norm,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_sizes)):\n",
    "            # Set the input size equal to the output size\n",
    "            n = hidden_sizes[i]\n",
    "\n",
    "            layers.append(\n",
    "                FCBlock(\n",
    "                    hidden_sizes[i],\n",
    "                    hidden_sizes[i],\n",
    "                    activation=activation,\n",
    "                    layer_norm=layer_norm,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diaz/anaconda3/envs/modal/lib/python3.10/site-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
      "/home/diaz/anaconda3/envs/modal/lib/python3.10/site-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return super().__sizeof__() + self.nbytes()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "FC                                       [1, 100]                  --\n",
       "├─Sequential: 1-1                        [1, 100]                  --\n",
       "│    └─FCBlock: 2-1                      [1, 100]                  --\n",
       "│    │    └─Linear: 3-1                  [1, 100]                  800\n",
       "│    │    └─Identity: 3-2                [1, 100]                  --\n",
       "│    └─FCBlock: 2-4                      --                        (recursive)\n",
       "│    │    └─LeakyReLU: 3-3               [1, 100]                  --\n",
       "│    └─FCBlock: 2-3                      [1, 100]                  --\n",
       "│    │    └─Linear: 3-4                  [1, 100]                  10,100\n",
       "│    │    └─Identity: 3-5                [1, 100]                  --\n",
       "│    └─FCBlock: 2-4                      --                        (recursive)\n",
       "│    │    └─LeakyReLU: 3-6               [1, 100]                  --\n",
       "│    └─FCBlock: 2-5                      [1, 100]                  --\n",
       "│    │    └─Linear: 3-7                  [1, 100]                  10,100\n",
       "│    │    └─Identity: 3-8                [1, 100]                  --\n",
       "│    │    └─LeakyReLU: 3-9               [1, 100]                  --\n",
       "│    └─Linear: 2-6                       [1, 100]                  10,100\n",
       "==========================================================================================\n",
       "Total params: 31,100\n",
       "Trainable params: 31,100\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.03\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.12\n",
       "Estimated Total Size (MB): 0.13\n",
       "=========================================================================================="
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = FC(\n",
    "    input_size=7,\n",
    "    output_size=100,\n",
    "    hidden_sizes=[100, 100],\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 7)\n",
    "y = model(x)\n",
    "\n",
    "assert y.shape == torch.Size([1, 100])\n",
    "summary(model, input_size=(1, 7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class CoefficientsFC(nn.Module):\n",
    "    \"\"\"\n",
    "    A wrapper around the FC class that outputs the coefficients of a filterbank\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_parallel: int = 32,\n",
    "        n_biquads: int = 2,\n",
    "        initial_gain_scale: float = 1.0,\n",
    "        tahn_c: float = 1.0,\n",
    "        tanh_d: float = 1.0,\n",
    "        use_zp_init: bool = False,\n",
    "        initial_pole_mag: float = 1.5,\n",
    "        initial_zero_mag: float = 0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_parallel = n_parallel\n",
    "        self.n_biquads = n_biquads\n",
    "        self.initial_gain_scale = initial_gain_scale\n",
    "        self.tahn_c = tahn_c\n",
    "        self.tanh_d = tanh_d\n",
    "        self.use_zp_init = use_zp_init\n",
    "\n",
    "        if use_zp_init:\n",
    "            z_offset = torch.polar(\n",
    "                torch.zeros(self.n_parallel, self.n_biquads) * initial_zero_mag,\n",
    "                torch.rand(self.n_parallel, self.n_biquads) * np.pi\n",
    "            )\n",
    "            p_offset = torch.polar(\n",
    "                torch.ones(self.n_parallel, self.n_biquads) * initial_pole_mag,\n",
    "                torch.rand(self.n_parallel, self.n_biquads) * np.pi\n",
    "            )\n",
    "            self.register_buffer(\"z_offset\", z_offset)\n",
    "            self.register_buffer(\"p_offset\", p_offset)\n",
    "\n",
    "        # override the output size\n",
    "        kwargs[\"output_size\"] = n_parallel * n_biquads * 5\n",
    "        self.fc = FC(**kwargs)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor, # A concatenated tensor of features, coodinates, and material properties\n",
    "    ) -> torch.Tensor: # The concatenated coefficients of the filterbank\n",
    "        \n",
    "        x = self.fc(x)\n",
    "\n",
    "        z, p, k = to_zpk(\n",
    "            x=x,\n",
    "            n_parallel=self.n_parallel,\n",
    "            n_biquads=self.n_biquads,\n",
    "            gain_factor=self.initial_gain_scale,\n",
    "        )\n",
    "\n",
    "        if self.use_zp_init:\n",
    "            z = self.z_offset + z\n",
    "            p = self.p_offset + p\n",
    "\n",
    "        b = pole_or_zero_to_iir_coeff(z) * k[..., None]\n",
    "        a = pole_or_zero_to_iir_coeff(\n",
    "            constrain_complex_pole_or_zero(\n",
    "                p,\n",
    "                c=self.tahn_c,\n",
    "                d=self.tanh_d,\n",
    "            )\n",
    "        )\n",
    "    \n",
    "        return torch.concat([b, a], dim=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_parallel = 32\n",
    "n_biquads = 2\n",
    "n_coeffs = 6\n",
    "n_batches = 2\n",
    "coefficients_fc = CoefficientsFC(\n",
    "    input_size=1007,\n",
    "    hidden_sizes=[100, 100],\n",
    "    n_parallel=n_parallel,\n",
    "    n_biquads=n_biquads,\n",
    "    initial_gain_scale=1.0,\n",
    ")\n",
    "\n",
    "x = torch.randn(n_batches, 1007)\n",
    "y = coefficients_fc(x)\n",
    "assert y.shape == torch.Size([n_batches, n_parallel, n_biquads, n_coeffs])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
