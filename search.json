[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "source\n\n\n\n SingleShapeDataset\n                     (material:neuralresonator.modal.Material=Material(rho\n                     =2700, E=72000000000.0, nu=0.19, alpha=6,\n                     beta=1e-07), n_modes:int=32,\n                     audio_length_in_seconds:float=0.3,\n                     sample_rate:float=32000, n_refinements:int=3,\n                     mesh:Optional[skfem.mesh.mesh_2d.Mesh2D]=None)\n\nA synthetic dataset of materials and audio generated from their modal vibrations.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmaterial\nMaterial\nMaterial(rho=2700, E=72000000000.0, nu=0.19, alpha=6, beta=1e-07)\nMaterial to use\n\n\nn_modes\nint\n32\nNumber of modes to use\n\n\naudio_length_in_seconds\nfloat\n0.3\nLength of audio to render\n\n\nsample_rate\nfloat\n32000\nSample rate of audio to render\n\n\nn_refinements\nint\n3\nNumber of refinement steps for the mesh\n\n\nmesh\ntyping.Optional[skfem.mesh.mesh_2d.Mesh2D]\nNone"
  },
  {
    "objectID": "data.html#datasets",
    "href": "data.html#datasets",
    "title": "Data",
    "section": "",
    "text": "source\n\n\n\n SingleShapeDataset\n                     (material:neuralresonator.modal.Material=Material(rho\n                     =2700, E=72000000000.0, nu=0.19, alpha=6,\n                     beta=1e-07), n_modes:int=32,\n                     audio_length_in_seconds:float=0.3,\n                     sample_rate:float=32000, n_refinements:int=3,\n                     mesh:Optional[skfem.mesh.mesh_2d.Mesh2D]=None)\n\nA synthetic dataset of materials and audio generated from their modal vibrations.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmaterial\nMaterial\nMaterial(rho=2700, E=72000000000.0, nu=0.19, alpha=6, beta=1e-07)\nMaterial to use\n\n\nn_modes\nint\n32\nNumber of modes to use\n\n\naudio_length_in_seconds\nfloat\n0.3\nLength of audio to render\n\n\nsample_rate\nfloat\n32000\nSample rate of audio to render\n\n\nn_refinements\nint\n3\nNumber of refinement steps for the mesh\n\n\nmesh\ntyping.Optional[skfem.mesh.mesh_2d.Mesh2D]\nNone"
  },
  {
    "objectID": "data.html#dataset-generator",
    "href": "data.html#dataset-generator",
    "title": "Data",
    "section": "Dataset generator",
    "text": "Dataset generator\n\nsource\n\ngenerate_dataset\n\n generate_dataset (materials:List[neuralresonator.modal.Material],\n                   shapes:List[numpy.ndarray], n_modes:int=32,\n                   n_refinements:int=3, scale_factor:float=1,\n                   resolution:tuple[int,int]=(64, 64),\n                   without_boundary_nodes:bool=True,\n                   data_dir:pathlib.Path=Path('data'))\n\nGenerates a dataset of shapes and materials.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmaterials\ntyping.List[neuralresonator.modal.Material]\n\nList of materials\n\n\nshapes\ntyping.List[numpy.ndarray]\n\nList of shapes\n\n\nn_modes\nint\n32\nNumber of modes\n\n\nn_refinements\nint\n3\nNumber of refinements\n\n\nscale_factor\nfloat\n1\nScale factor for the mesh\n\n\nresolution\ntuple\n(64, 64)\nResolution of the occupancy map\n\n\nwithout_boundary_nodes\nbool\nTrue\nWhether to remove the boundary nodes\n\n\ndata_dir\nPath\ndata\nDirectory to save the data\n\n\n\n\nsource\n\n\ngenerate_random_dataset\n\n generate_random_dataset (n_shapes:int, n_materials:int, n_vertices:int,\n                          **kwargs)\n\nGenerates a dataset of shapes and materials.\n\n\n\n\nType\nDetails\n\n\n\n\nn_shapes\nint\nNumber of shapes to generate\n\n\nn_materials\nint\nNumber of materials to generate\n\n\nn_vertices\nint\nNumber of vertices for the shape\n\n\nkwargs\n\n\n\n\n\nGenerate a random dataset\n\nn_shapes = 10\nn_materials = 1\nn_vertices = 13\nscale_factor = 1\nn_refinements = 3\n\ndata_dir = Path(\"data\")\n\nif not data_dir.exists():\n    data_dir.mkdir()\n\ngenerate_random_dataset(\n    n_shapes=n_shapes,\n    n_materials=n_materials,\n    materials=[MATERIALS['polycarbonate']],\n    n_vertices=n_vertices,\n    n_modes=64,\n    scale_factor=scale_factor,\n    n_refinements=n_refinements,\n)\n\nFinish generating shapes and materials\n\n\nSaved 00009_00000.npy: 100%|██████████| 10/10 [00:06&lt;00:00,  1.55it/s]\n\n\n\nsource\n\n\nMultiShapeMultiMaterialDataset\n\n MultiShapeMultiMaterialDataset (index_map_path:pathlib.Path,\n                                 audio_length_in_seconds:float=0.3,\n                                 sample_rate:int=16000)\n\nA synthetic dataset of materials, shapes and audio generated from their modal vibrations.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nindex_map_path\nPath\n\nDirectory to read data from\n\n\naudio_length_in_seconds\nfloat\n0.3\nLength of audio to render\n\n\nsample_rate\nint\n16000\nSample rate of audio to render\n\n\nReturns\nNone\n\n\n\n\n\nLoad a dataset\n\nsr = 16000\ndataset = MultiShapeMultiMaterialDataset(\n    Path(\"data/index_map.csv\"),\n    sample_rate=sr,\n)\ndata = dataset[np.random.choice(len(dataset))]\n\nmask = data[\"mask\"]\nscaled_coords = data[\"coords\"] * mask.shape[1]\nmaterial = data[\"material_params\"]\naudio = data[\"audio\"]\nprint(audio.shape)\nprint(f\"Material: {material}\")\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nax[0].imshow(mask[0])\nax[0].scatter(scaled_coords[0], scaled_coords[1], c=\"r\")\nax[1].plot(audio)\nax[1].set_title(\"Audio\")\nplt.show()\n\nsave_and_display_audio(audio, f\"dataload.wav\", sr)\n\n(4800,)\nMaterial: [ 0.07263158  0.0014014   0.74       -0.01724138  0.1959799 ]\n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "data.html#data-module",
    "href": "data.html#data-module",
    "title": "Data",
    "section": "Data module",
    "text": "Data module\n\nsource\n\nMultiShapeMultiMaterialDataModule\n\n MultiShapeMultiMaterialDataModule\n                                    (train_index_map_path:Union[pathlib.Pa\n                                    th,str], val_index_map_path:Union[path\n                                    lib.Path,str], test_index_map_path:Uni\n                                    on[pathlib.Path,str],\n                                    audio_length_in_seconds:float=0.3,\n                                    sample_rate:int=16000, material_ranges\n                                    :neuralresonator.modal.MaterialRanges=\n                                    MaterialRanges(rho=(500.0, 10000.0),\n                                    E=(1000000000.0, 1000000000000.0),\n                                    nu=(0.0, 0.5), alpha=(1.0, 30.0),\n                                    beta=(1e-08, 2e-06)),\n                                    batch_size:int=1, num_workers:int=0,\n                                    pin_memory:bool=False)\n\nA data module for the MultiShapeMultiMaterialDataset\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntrain_index_map_path\ntyping.Union[pathlib.Path, str]\n\nDirectory containing training data\n\n\nval_index_map_path\ntyping.Union[pathlib.Path, str]\n\nDirectory containing validation data\n\n\ntest_index_map_path\ntyping.Union[pathlib.Path, str]\n\nDirectory containing test data\n\n\naudio_length_in_seconds\nfloat\n0.3\nLength of audio to render\n\n\nsample_rate\nint\n16000\nSample rate of audio to render\n\n\nmaterial_ranges\nMaterialRanges\nMaterialRanges(rho=(500.0, 10000.0), E=(1000000000.0, 1000000000000.0), nu=(0.0, 0.5), alpha=(1.0, 30.0), beta=(1e-08, 2e-06))\nRanges of material parameters\n\n\nbatch_size\nint\n1\nBatch size\n\n\nnum_workers\nint\n0\nNumber of workers for data loading\n\n\npin_memory\nbool\nFalse\nWhether to pin memory for data loading\n\n\nReturns\nNone\n\n\n\n\n\nCreate a data module\n\ndataset_args = dict(\n    audio_length_in_seconds=0.3,\n    sample_rate=16000,\n)\n\ndatamodule = MultiShapeMultiMaterialDataModule(\n    train_index_map_path=\"data/index_map.csv\",\n    val_index_map_path=\"data/index_map.csv\",\n    test_index_map_path=\"data/index_map.csv\",\n)"
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "Results",
    "section": "",
    "text": "Physical models of rigid bodies are used for sound synthesis in applications from virtual environments to music production. Traditional methods such as modal synthesis often rely on computationally expensive numerical solvers, while recent deep learning approaches are limited by post-processing of their results. In this work we present a novel end-to-end framework for training a deep neural network to generate modal resonators for a given 2D shape and material, using a bank of differentiable IIR filters. We demonstrate our method on a dataset of synthetic objects, but train our model using an audio-domain objective, paving the way for physically-informed synthesisers to be learned directly from recordings of real-world objects.\nCompare the original audio and the generated audio.\n\n\n\n\n\n\n\n\n\n\n\nRandom shape/material\n\n\n\n\n\n\n\nSpectrogram original\n\n\n\n\n\n\n\nSpectrogram predicted\n\n\n\n\n\n\n\nAudio original\nAudio\nAudio\nAudio\nAudio\nAudio\n\n\nAudio predicted\nAudio\nAudio\nAudio\nAudio\nAudio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDensity\nOriginal\nPredicted\n\n\n\n\n\\(\\rho=500.00\\)\nAudio\nAudio\n\n\n\\(\\rho=2111.11\\)\nAudio\nAudio\n\n\n\\(\\rho=3722.22\\)\nAudio\nAudio\n\n\n\\(\\rho=5333.33\\)\nAudio\nAudio\n\n\n\\(\\rho=6944.44\\)\nAudio\nAudio\n\n\n\\(\\rho=8555.56\\)\nAudio\nAudio\n\n\n\\(\\rho=10166.67\\)\nAudio\nAudio\n\n\n\\(\\rho=11777.78\\)\nAudio\nAudio\n\n\n\\(\\rho=13388.89\\)\nAudio\nAudio\n\n\n\\(\\rho=15000.00\\)\nAudio\nAudio\n\n\n\n\n\n\n\n\n\n\n\nPoisson’s ratio\nOriginal\nPredicted\n\n\n\n\n\\(\\nu=0.10\\)\nAudio\nAudio\n\n\n\\(\\nu=0.14\\)\nAudio\nAudio\n\n\n\\(\\nu=0.19\\)\nAudio\nAudio\n\n\n\\(\\nu=0.23\\)\nAudio\nAudio\n\n\n\\(\\nu=0.27\\)\nAudio\nAudio\n\n\n\\(\\nu=0.32\\)\nAudio\nAudio\n\n\n\\(\\nu=0.36\\)\nAudio\nAudio\n\n\n\\(\\nu=0.40\\)\nAudio\nAudio\n\n\n\\(\\nu=0.45\\)\nAudio\nAudio\n\n\n\\(\\nu=0.49\\)\nAudio\nAudio\n\n\n\nInterpolate between two shapes, and see how the audio changes.\n\nIn this case, we do not have discretized positions for the coordinates in the shapes (as FEM requires discretization). Because our network acts as a neural field we can obtain sound for continuous coordinate values. We can interpolate between the two shapes and see how the audio changes.\n\n\n\n\n\n\n\n\nShape\nPredicted sound\n\n\n\n\n\nAudio\n\n\n\nAudio\n\n\n\nAudio\n\n\n\nAudio\n\n\n\nAudio\n\n\n\nAudio\n\n\n\nAudio\n\n\n\nAudio\n\n\n\nAudio\n\n\n\nAudio"
  },
  {
    "objectID": "shape.html",
    "href": "shape.html",
    "title": "Shape",
    "section": "",
    "text": "source\n\ngenerate_convex_mesh\n\n generate_convex_mesh (n_points:int=10, n_refinement_steps:int=3)\n\nGenerate a 2D convex mesh\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_points\nint\n10\nnumber of points in the convex shape\n\n\nn_refinement_steps\nint\n3\nnumber of refinement steps\n\n\nReturns\ntuple\n\nmesh and points in the normalized range [-1.0, 1.0]\n\n\n\n\nsource\n\n\ngenerate_convex_shape\n\n generate_convex_shape (n:int)\n\nThis function was taken and adapted from https://stackoverflow.com/a/68602707\nGenerate convex shappes according to Pavel Valtr’s 1995 alogrithm. Ported from Sander Verdonschot’s Java version, found here: https://cglab.ca/~sander/misc/ConvexGeneration/ValtrAlgorithm.java\nreturns a numpy array of shape (n, 2)\n\ndef rasterize(\n    side: int,  # side of the image\n    points: np.ndarray,  # points defining the shape\n) -&gt; np.ndarray:  # rasterized shape as a boolean array\n    \"\"\"\n    Rasterize a shape defined by a set of points\n    \"\"\"\n    return polygon2mask((side, side), ((points + 1.0) / 2.0) * side).T\n\nGenerate random convex polygons and rasterize them.\n\nmesh, points = generate_convex_mesh(10, 3)\n\nsystem = System(MATERIALS['polycarbonate'], mesh)\neigenvalues = system.eigenvalues\neigenvectors = system.get_mode_gains()\n\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\n\nMeshTri.draw(mesh, shading={\"wireframe\": True}, ax=ax[0])\nax[0].set_xlim(-1, 1)\nax[0].set_ylim(-1, 1)\nax[0].set_title(\"Mesh\")\nax[0].set_xlabel(\"normalized x\")\nax[0].set_ylabel(\"normalized y\")\n\nmask = rasterize(128, points)\nax[1].imshow(mask, origin=\"lower\")\nax[1].set_title(\"Rasterized shape\")\nax[1].set_xlabel(\"pixels x\")\nax[1].set_ylabel(\"pixels y\")\n\nText(0, 0.5, 'pixels y')"
  },
  {
    "objectID": "scripts.html",
    "href": "scripts.html",
    "title": "Scripts",
    "section": "",
    "text": "source\n\n\n\n train (cfg:omegaconf.dictconfig.DictConfig)\n\n\nsource\n\n\n\n\n log_hyperparameters (object_dict:dict)\n\nLog hyperparameters to all loggers.\n\n# import os\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = 3\n\nwith initialize(version_base=None, config_path=\"../configs\"):\n    cfg = compose(\n        config_name=\"train.yaml\",\n        return_hydra_config=True,\n        overrides=[\n            \"trainer.max_epochs=1\",\n            \"hydra.runtime.output_dir=outputs\",\n            \"paths.output_dir=${hydra.runtime.output_dir}\",\n            \"paths.work_dir=${hydra.runtime.cwd}\",\n            \"seed=42\",\n            \"logger=null\",\n            \"++datamodule.train_index_map_path=data/index_map.csv\",\n            \"++datamodule.val_index_map_path=data/index_map.csv\",\n            \"++datamodule.test_index_map_path=data/index_map.csv\",\n        ],\n    )\n    train(cfg)"
  },
  {
    "objectID": "scripts.html#export",
    "href": "scripts.html#export",
    "title": "Scripts",
    "section": "Export",
    "text": "Export\n\n@hydra.main(version_base=None, config_path=\"../configs\", config_name=\"export\")\ndef export(\n    cfg: DictConfig,\n):\n\n    # Load checkpoint\n    model = MultiShapeMultiMaterialLitModule.load_from_checkpoint(cfg.ckpt_path)\n    model.eval()\n\n    # export encoder to torchscript\n    script = torch.jit.script(model.encoder)\n    torch.jit.save(script, cfg.encoder_path)\n\n    # export coefficient model to torchscript\n    script = torch.jit.script(model.model)\n    torch.jit.save(script, cfg.coefficient_model_path)"
  },
  {
    "objectID": "training.html",
    "href": "training.html",
    "title": "Training",
    "section": "",
    "text": "source\n\nMultiShapeMultiMaterialLitModule\n\n MultiShapeMultiMaterialLitModule (model:torch.nn.modules.module.Module,\n                                   optimizer:Type[torch.optim.optimizer.Op\n                                   timizer], scheduler:Type[torch.optim.lr\n                                   _scheduler.LRScheduler], criterion:torc\n                                   h.nn.modules.module.Module=FFTLoss(),\n                                   **kwargs)\n\nHooks to be used in LightningModule.\nTry to run a single batch\n\nfrom neuralresonator.data import MultiShapeMultiMaterialDataModule\nfrom neuralresonator.models import FC\nfrom dataclasses import dataclass\nfrom omegaconf import DictConfig, OmegaConf\nfrom hydra.utils import instantiate\n\ndataset_args = dict()\n\ndatamodule = MultiShapeMultiMaterialDataModule(\n    train_index_map_path=\"data/index_map.csv\",\n    val_index_map_path=\"data/index_map.csv\",\n    test_index_map_path=\"data/index_map.csv\",\n)\n\ncfg = OmegaConf.create(\n    {\n        \"_target_\": \"neuralresonator.training.MultiShapeMultiMaterialLitModule\",\n        \"model\": {\n            \"_target_\": \"neuralresonator.models.CoefficientsFC\",\n            \"input_size\": 1007,\n            \"hidden_sizes\": [1024, 1024, 1024, 1024, 1024, 1024],\n            \"n_parallel\": 32,\n            \"n_biquads\": 2,\n        },\n\n        \"criterion\": {\n            \"_target_\": \"neuralresonator.utilities.FFTLoss\",\n        },\n        \"optimizer\": {\n            \"_target_\": \"torch.optim.Adam\",\n            \"_partial_\": True,\n            \"lr\": 0.0001,\n        },\n        \"scheduler\": {\n            \"_target_\": \"torch.optim.lr_scheduler.ExponentialLR\",\n            \"_partial_\": True,\n            \"gamma\": 0.999,\n            \"verbose\": True,\n        },\n    }\n)\nfrom lightning.pytorch import loggers\n\nmodel = instantiate(cfg)\nlogger = loggers.WandbLogger(project=\"neuralresonator\")\n\ntrainer = pl.Trainer(\n    limit_train_batches=1,\n    max_epochs=1,\n    limit_val_batches=1,\n    logger=logger,\n)\n\ntrainer.fit(model=model, datamodule=datamodule)\n\n/home/diaz/anaconda3/envs/modal/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:197: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n  rank_zero_warn(\n/home/diaz/anaconda3/envs/modal/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:197: UserWarning: Attribute 'criterion' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['criterion'])`.\n  rank_zero_warn(\n/home/diaz/anaconda3/envs/modal/lib/python3.10/site-packages/lightning/pytorch/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n  rank_zero_warn(\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n`Trainer(limit_train_batches=1)` was configured so 1 batch per epoch will be used.\n`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n/home/diaz/anaconda3/envs/modal/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory ./neuralresonator/fm3vnhcd/checkpoints exists and is not empty.\n  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name      | Type              | Params\n------------------------------------------------\n0 | model     | CoefficientsFC    | 7.7 M \n1 | criterion | FFTLoss           | 0     \n2 | encoder   | EfficientNet      | 5.3 M \n3 | mse       | MeanSquaredError  | 0     \n4 | mae       | MeanAbsoluteError | 0     \n------------------------------------------------\n12.9 M    Trainable params\n0         Non-trainable params\n12.9 M    Total params\n51.785    Total estimated model params size (MB)\n/home/diaz/anaconda3/envs/modal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n  rank_zero_warn(\n/home/diaz/anaconda3/envs/modal/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n  rank_zero_warn(\n/home/diaz/anaconda3/envs/modal/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n  rank_zero_warn(\n`Trainer.fit` stopped: `max_epochs=1` reached.\n\n\nAdjusting learning rate of group 0 to 1.0000e-04.\n\n\n\n\n\n\n\n\n\n\n\n\n# Checkpointing\nprint(f\"Model hparams: {model.hparams}\")\ntrainer.save_checkpoint(\"checkpoint.ckpt\")\n\n# Load checkpoint\nmodel = MultiShapeMultiMaterialLitModule.load_from_checkpoint(\"checkpoint.ckpt\")\n\nModel hparams: \"criterion\": FFTLoss()\n\"model\":     CoefficientsFC(\n  (fc): FC(\n    (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n    (network): Sequential(\n      (0): FCBlock(\n        (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n        (linear): Linear(in_features=1007, out_features=1024, bias=True)\n        (ln): Identity()\n      )\n      (1): FCBlock(\n        (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n        (linear): Linear(in_features=1024, out_features=1024, bias=True)\n        (ln): Identity()\n      )\n      (2): FCBlock(\n        (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n        (linear): Linear(in_features=1024, out_features=1024, bias=True)\n        (ln): Identity()\n      )\n      (3): FCBlock(\n        (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n        (linear): Linear(in_features=1024, out_features=1024, bias=True)\n        (ln): Identity()\n      )\n      (4): FCBlock(\n        (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n        (linear): Linear(in_features=1024, out_features=1024, bias=True)\n        (ln): Identity()\n      )\n      (5): FCBlock(\n        (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n        (linear): Linear(in_features=1024, out_features=1024, bias=True)\n        (ln): Identity()\n      )\n      (6): FCBlock(\n        (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n        (linear): Linear(in_features=1024, out_features=1024, bias=True)\n        (ln): Identity()\n      )\n      (7): Linear(in_features=1024, out_features=320, bias=True)\n    )\n  )\n)\n\"optimizer\": functools.partial(&lt;class 'torch.optim.adam.Adam'&gt;, lr=0.0001)\n\"scheduler\": functools.partial(&lt;class 'torch.optim.lr_scheduler.ExponentialLR'&gt;, gamma=0.999, verbose=True)\n\n\n/home/diaz/anaconda3/envs/modal/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:197: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n  rank_zero_warn(\n/home/diaz/anaconda3/envs/modal/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:197: UserWarning: Attribute 'criterion' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['criterion'])`.\n  rank_zero_warn("
  },
  {
    "objectID": "dsp.html",
    "href": "dsp.html",
    "title": "DSP",
    "section": "",
    "text": "source\n\n\n\n pole_or_zero_to_iir_coeff (pole_or_zero:torch.Tensor)\n\nConverts a complex pole or zero to IIR coefficients\n\nsource\n\n\n\n\n constrain_complex_pole_or_zero (pole_or_zero:torch.Tensor,\n                                 floor_eps:float=1e-10, ceil_eps:float=0,\n                                 c:float=1.0, d:float=1.0)\n\nSaturates the absolute value of the complex pole or zero as it approaches the unit circle, preventing filter instability.\n\nsource\n\n\n\n\n mtanh (x:torch.Tensor, c:float=1.0, d:float=1.0)\n\n\nsource\n\n\n\n\n biquad_freqz (b:torch.Tensor, a:torch.Tensor,\n               n:Union[int,torch.Tensor]=512)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nb\nTensor\n\nTensor of shape (…, 3)\n\n\na\nTensor\n\nTensor of shape (…, 3)\n\n\nn\ntyping.Union[int, torch.Tensor]\n512\nNumber of points to evaluate the frequency response at\n\n\nReturns\nTensor\n\nTensor of shape (…, n // 2 + 1)\n\n\n\n\n# create a pole zero pair\npole = torch.complex(real=torch.tensor(0.5), imag=torch.tensor(0.3))\nzero = torch.complex(real=torch.tensor(0.8), imag=torch.tensor(-0.4))\n\n# convert to IIR coefficients\nb = pole_or_zero_to_iir_coeff(zero)\na = pole_or_zero_to_iir_coeff(constrain_complex_pole_or_zero(pole))\n\n# compute magnitude response\nH = biquad_freqz(b, a, 129)\nH_mag = H.abs()\n\n# plot\nplt.stem(H_mag)\n\nH.shape: torch.Size([65])\n\n\n&lt;StemContainer object of 3 artists&gt;\n\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n IIRMethod (value, names=None, module=None, qualname=None, type=None,\n            start=1)\n\nAn enumeration over IIR computation backends. Available options:\n\nIIRMethod.FFT: Uses the FFT frequency sampling method.\nIIRMethod.TDFII: Uses the TDF-II algorithm.\n\n\nsource\n\n\n\n\n IIRParameters (b:Optional[torch.Tensor]=None,\n                a:Optional[torch.Tensor]=None,\n                poles:Optional[torch.Tensor]=None,\n                zeros:Optional[torch.Tensor]=None,\n                gains:Optional[torch.Tensor]=None,\n                constrain_poles:bool=True, constrain_zeros:bool=True,\n                constraint_eps:float=0.0)\n\nA class for storing IIR filter parameters.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nb\ntyping.Optional[torch.Tensor]\nNone\n\n\n\na\ntyping.Optional[torch.Tensor]\nNone\nThe numerator coefficients (…, n_parallel, n_biquad, 3)\n\n\npoles\ntyping.Optional[torch.Tensor]\nNone\nThe denominator coefficients (…, n_parallel, n_biquad, 3)\n\n\nzeros\ntyping.Optional[torch.Tensor]\nNone\nThe complex poles (…, n_parallel, n_biquad)\n\n\ngains\ntyping.Optional[torch.Tensor]\nNone\nThe complex zeros (…, n_parallel, n_biquad)\n\n\nconstrain_poles\nbool\nTrue\nWhether to constrain the poles to the unit circle\n\n\nconstrain_zeros\nbool\nTrue\nWhether to constrain the zeros to the unit circle\n\n\nconstraint_eps\nfloat\n0.0\nThe epsilon value to use for pole/zero constraint\n\n\n\nWe can demonstrate hybrid freqz computation by creating a network of passthrough filters:\n\nn_biquads = 15\nn_parallel = 8\nn_samples = 1000\n\nb = torch.zeros(n_parallel, n_biquads, 3)\nb[:, :, 0] = 1.0\na = torch.zeros(n_parallel, n_biquads, 3)\na[:, :, 0] = 1.0\n\ngains = torch.ones(n_parallel, n_biquads) / (n_parallel ** (1 / n_biquads))\nparams = IIRParameters(b=b, a=a, gains=gains)\n\nplt.stem(params.freqz(128).abs())\nplt.show()\n\n\n\n\n\nsource\n\n\n\n\n apply_iir (x:torch.Tensor, parameters:__main__.IIRParameters,\n            method:__main__.IIRMethod=&lt;function _apply_biquad_fft&gt;,\n            expand_x:bool=True, reduce_x:bool=True)\n\nApplies a biquad filter to a signal. The filter can be specified in two ways:\n\nDirectly as the biquad coefficients (b, a)\nAs the complex poles and zeros (poles, zeros)\n\nIn both cases, the filter can be specified as a parallel (n_biquad = 1), cascade (n_parallel = 1), or hybrid ((n_biquad &gt; 1) and (n_parallel &gt; 1)).\nThe method parameter specifies the method to use for filtering. The FFT method is computed without recursion by frequency sampling, while the TDFII method is computed recursively using the transposed direct form II structure.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nTensor\n\nThe input signal (…, time)\n\n\nparameters\nIIRParameters\n\nThe IIR filter parameters\n\n\nmethod\nIIRMethod\n_apply_biquad_fft\nThe method to use for filtering\n\n\nexpand_x\nbool\nTrue\nWhether to expand the input signal to match the number of parallel filters\n\n\nreduce_x\nbool\nTrue\nWhether to sum the output signal along the parallel filter dimension\n\n\n\nTest that expected shape is returned:\n\nn_biquads = 2\nn_parallel = 9\nn_samples = 1000\nbatch_size = 4\n\npoles = torch.exp(-1j * torch.rand(batch_size, n_parallel, n_biquads) * np.pi)\nzeros = torch.exp(-1j * torch.rand(batch_size, n_parallel, n_biquads) * np.pi)\ngains = torch.rand(batch_size, n_parallel, n_biquads)\nparams = IIRParameters(poles=poles, zeros=zeros, gains=gains)\n\nx = torch.rand(batch_size, n_samples)\n\nassert apply_iir(x, params, IIRMethod.FFT).shape == x.shape\n\n\nn_biquads = 2\nn_parallel = 9\nn_samples = 1000\nbatch_size = 4\n\nb = torch.rand(batch_size, n_parallel, n_biquads, 3)\na = torch.rand(batch_size, n_parallel, n_biquads, 3)\ngains = None\nparams = IIRParameters(b=b, a=a, gains=gains)\n\nx = torch.rand(batch_size, n_samples)\n\nassert apply_iir(x, params, IIRMethod.TDFII).shape == x.shape\n\nWe can also have an optimized version of the above\n\nsource\n\n\n\n\n apply_filter (x_arr:numpy.ndarray, a:numpy.ndarray, b:numpy.ndarray,\n               state:Optional[numpy.ndarray]=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx_arr\nndarray\n\n[n_samples]\n\n\na\nndarray\n\n[n_parallel, n_biquads, 3]\n\n\nb\nndarray\n\n[n_parallel, n_biquads, 3]\n\n\nstate\ntyping.Optional[numpy.ndarray]\nNone\n[n_parallel, n_biquads, 2]\n\n\n\nTest fast filter\n\n# create a pole zero pair\npole = torch.complex(real=torch.tensor(0.5), imag=torch.tensor(0.3))\nzero = torch.complex(real=torch.tensor(0.8), imag=torch.tensor(-0.4))\n\n# convert to IIR coefficients\nb = pole_or_zero_to_iir_coeff(zero)\na = pole_or_zero_to_iir_coeff(constrain_complex_pole_or_zero(pole))\n\nb = b.view(1, 1, 3)\na = a.view(1, 1, 3)\n\nimpulse = torch.zeros(256)\nimpulse[0] = 1.0\n\ngains = None\nparams = IIRParameters(b=b, a=a)\nfiltered_torch = apply_iir(impulse.unsqueeze(0), params, IIRMethod.TDFII)\n\nfiltered = apply_filter(\n    x_arr=impulse.cpu().numpy(),\n    a = a.cpu().numpy(),\n    b = b.cpu().numpy(),\n)\n\n# check that the output is the same\nassert np.allclose(filtered, filtered_torch[0].detach().cpu().numpy())"
  },
  {
    "objectID": "dsp.html#utility-functions",
    "href": "dsp.html#utility-functions",
    "title": "DSP",
    "section": "",
    "text": "source\n\n\n\n pole_or_zero_to_iir_coeff (pole_or_zero:torch.Tensor)\n\nConverts a complex pole or zero to IIR coefficients\n\nsource\n\n\n\n\n constrain_complex_pole_or_zero (pole_or_zero:torch.Tensor,\n                                 floor_eps:float=1e-10, ceil_eps:float=0,\n                                 c:float=1.0, d:float=1.0)\n\nSaturates the absolute value of the complex pole or zero as it approaches the unit circle, preventing filter instability.\n\nsource\n\n\n\n\n mtanh (x:torch.Tensor, c:float=1.0, d:float=1.0)\n\n\nsource\n\n\n\n\n biquad_freqz (b:torch.Tensor, a:torch.Tensor,\n               n:Union[int,torch.Tensor]=512)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nb\nTensor\n\nTensor of shape (…, 3)\n\n\na\nTensor\n\nTensor of shape (…, 3)\n\n\nn\ntyping.Union[int, torch.Tensor]\n512\nNumber of points to evaluate the frequency response at\n\n\nReturns\nTensor\n\nTensor of shape (…, n // 2 + 1)\n\n\n\n\n# create a pole zero pair\npole = torch.complex(real=torch.tensor(0.5), imag=torch.tensor(0.3))\nzero = torch.complex(real=torch.tensor(0.8), imag=torch.tensor(-0.4))\n\n# convert to IIR coefficients\nb = pole_or_zero_to_iir_coeff(zero)\na = pole_or_zero_to_iir_coeff(constrain_complex_pole_or_zero(pole))\n\n# compute magnitude response\nH = biquad_freqz(b, a, 129)\nH_mag = H.abs()\n\n# plot\nplt.stem(H_mag)\n\nH.shape: torch.Size([65])\n\n\n&lt;StemContainer object of 3 artists&gt;"
  },
  {
    "objectID": "dsp.html#main-interface",
    "href": "dsp.html#main-interface",
    "title": "DSP",
    "section": "",
    "text": "source\n\n\n\n IIRMethod (value, names=None, module=None, qualname=None, type=None,\n            start=1)\n\nAn enumeration over IIR computation backends. Available options:\n\nIIRMethod.FFT: Uses the FFT frequency sampling method.\nIIRMethod.TDFII: Uses the TDF-II algorithm.\n\n\nsource\n\n\n\n\n IIRParameters (b:Optional[torch.Tensor]=None,\n                a:Optional[torch.Tensor]=None,\n                poles:Optional[torch.Tensor]=None,\n                zeros:Optional[torch.Tensor]=None,\n                gains:Optional[torch.Tensor]=None,\n                constrain_poles:bool=True, constrain_zeros:bool=True,\n                constraint_eps:float=0.0)\n\nA class for storing IIR filter parameters.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nb\ntyping.Optional[torch.Tensor]\nNone\n\n\n\na\ntyping.Optional[torch.Tensor]\nNone\nThe numerator coefficients (…, n_parallel, n_biquad, 3)\n\n\npoles\ntyping.Optional[torch.Tensor]\nNone\nThe denominator coefficients (…, n_parallel, n_biquad, 3)\n\n\nzeros\ntyping.Optional[torch.Tensor]\nNone\nThe complex poles (…, n_parallel, n_biquad)\n\n\ngains\ntyping.Optional[torch.Tensor]\nNone\nThe complex zeros (…, n_parallel, n_biquad)\n\n\nconstrain_poles\nbool\nTrue\nWhether to constrain the poles to the unit circle\n\n\nconstrain_zeros\nbool\nTrue\nWhether to constrain the zeros to the unit circle\n\n\nconstraint_eps\nfloat\n0.0\nThe epsilon value to use for pole/zero constraint\n\n\n\nWe can demonstrate hybrid freqz computation by creating a network of passthrough filters:\n\nn_biquads = 15\nn_parallel = 8\nn_samples = 1000\n\nb = torch.zeros(n_parallel, n_biquads, 3)\nb[:, :, 0] = 1.0\na = torch.zeros(n_parallel, n_biquads, 3)\na[:, :, 0] = 1.0\n\ngains = torch.ones(n_parallel, n_biquads) / (n_parallel ** (1 / n_biquads))\nparams = IIRParameters(b=b, a=a, gains=gains)\n\nplt.stem(params.freqz(128).abs())\nplt.show()\n\n\n\n\n\nsource\n\n\n\n\n apply_iir (x:torch.Tensor, parameters:__main__.IIRParameters,\n            method:__main__.IIRMethod=&lt;function _apply_biquad_fft&gt;,\n            expand_x:bool=True, reduce_x:bool=True)\n\nApplies a biquad filter to a signal. The filter can be specified in two ways:\n\nDirectly as the biquad coefficients (b, a)\nAs the complex poles and zeros (poles, zeros)\n\nIn both cases, the filter can be specified as a parallel (n_biquad = 1), cascade (n_parallel = 1), or hybrid ((n_biquad &gt; 1) and (n_parallel &gt; 1)).\nThe method parameter specifies the method to use for filtering. The FFT method is computed without recursion by frequency sampling, while the TDFII method is computed recursively using the transposed direct form II structure.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nTensor\n\nThe input signal (…, time)\n\n\nparameters\nIIRParameters\n\nThe IIR filter parameters\n\n\nmethod\nIIRMethod\n_apply_biquad_fft\nThe method to use for filtering\n\n\nexpand_x\nbool\nTrue\nWhether to expand the input signal to match the number of parallel filters\n\n\nreduce_x\nbool\nTrue\nWhether to sum the output signal along the parallel filter dimension\n\n\n\nTest that expected shape is returned:\n\nn_biquads = 2\nn_parallel = 9\nn_samples = 1000\nbatch_size = 4\n\npoles = torch.exp(-1j * torch.rand(batch_size, n_parallel, n_biquads) * np.pi)\nzeros = torch.exp(-1j * torch.rand(batch_size, n_parallel, n_biquads) * np.pi)\ngains = torch.rand(batch_size, n_parallel, n_biquads)\nparams = IIRParameters(poles=poles, zeros=zeros, gains=gains)\n\nx = torch.rand(batch_size, n_samples)\n\nassert apply_iir(x, params, IIRMethod.FFT).shape == x.shape\n\n\nn_biquads = 2\nn_parallel = 9\nn_samples = 1000\nbatch_size = 4\n\nb = torch.rand(batch_size, n_parallel, n_biquads, 3)\na = torch.rand(batch_size, n_parallel, n_biquads, 3)\ngains = None\nparams = IIRParameters(b=b, a=a, gains=gains)\n\nx = torch.rand(batch_size, n_samples)\n\nassert apply_iir(x, params, IIRMethod.TDFII).shape == x.shape\n\nWe can also have an optimized version of the above\n\nsource\n\n\n\n\n apply_filter (x_arr:numpy.ndarray, a:numpy.ndarray, b:numpy.ndarray,\n               state:Optional[numpy.ndarray]=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx_arr\nndarray\n\n[n_samples]\n\n\na\nndarray\n\n[n_parallel, n_biquads, 3]\n\n\nb\nndarray\n\n[n_parallel, n_biquads, 3]\n\n\nstate\ntyping.Optional[numpy.ndarray]\nNone\n[n_parallel, n_biquads, 2]\n\n\n\nTest fast filter\n\n# create a pole zero pair\npole = torch.complex(real=torch.tensor(0.5), imag=torch.tensor(0.3))\nzero = torch.complex(real=torch.tensor(0.8), imag=torch.tensor(-0.4))\n\n# convert to IIR coefficients\nb = pole_or_zero_to_iir_coeff(zero)\na = pole_or_zero_to_iir_coeff(constrain_complex_pole_or_zero(pole))\n\nb = b.view(1, 1, 3)\na = a.view(1, 1, 3)\n\nimpulse = torch.zeros(256)\nimpulse[0] = 1.0\n\ngains = None\nparams = IIRParameters(b=b, a=a)\nfiltered_torch = apply_iir(impulse.unsqueeze(0), params, IIRMethod.TDFII)\n\nfiltered = apply_filter(\n    x_arr=impulse.cpu().numpy(),\n    a = a.cpu().numpy(),\n    b = b.cpu().numpy(),\n)\n\n# check that the output is the same\nassert np.allclose(filtered, filtered_torch[0].detach().cpu().numpy())"
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "Models",
    "section": "",
    "text": "source\n\nFC\n\n FC (input_size:int, output_size:int, hidden_sizes:List[int],\n     activation:torch.nn.modules.module.Module=LeakyReLU(negative_slope=0.\n     2, inplace=True), layer_norm:bool=False)\n\nA fully connected network with an activation function and optional layer normalization\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninput_size\nint\n\nThe size of the input\n\n\noutput_size\nint\n\nThe size of the output\n\n\nhidden_sizes\ntyping.List[int]\n\nThe sizes of the hidden layers\n\n\nactivation\nModule\nLeakyReLU(negative_slope=0.2, inplace=True)\n\n\n\nlayer_norm\nbool\nFalse\nWhether to use layer normalization\n\n\n\n\nsource\n\n\nFCBlock\n\n FCBlock (input_size:int, output_size:int,\n          activation:torch.nn.modules.module.Module=LeakyReLU(negative_slo\n          pe=0.2, inplace=True), layer_norm:bool=False)\n\nA fully connected block with an activation function and optional layer normalization\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninput_size\nint\n\nThe size of the input\n\n\noutput_size\nint\n\nThe size of the output\n\n\nactivation\nModule\nLeakyReLU(negative_slope=0.2, inplace=True)\n\n\n\nlayer_norm\nbool\nFalse\nWhether to use layer normalization\n\n\n\nTest\n\nmodel = FC(\n    input_size=7,\n    output_size=100,\n    hidden_sizes=[100, 100],\n)\n\nx = torch.randn(1, 7)\ny = model(x)\n\nassert y.shape == torch.Size([1, 100])\nsummary(model, input_size=(1, 7))\n\n/home/diaz/anaconda3/envs/modal/lib/python3.10/site-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  action_fn=lambda data: sys.getsizeof(data.storage()),\n/home/diaz/anaconda3/envs/modal/lib/python3.10/site-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return super().__sizeof__() + self.nbytes()\n\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nFC                                       [1, 100]                  --\n├─Sequential: 1-1                        [1, 100]                  --\n│    └─FCBlock: 2-1                      [1, 100]                  --\n│    │    └─Linear: 3-1                  [1, 100]                  800\n│    │    └─Identity: 3-2                [1, 100]                  --\n│    └─FCBlock: 2-4                      --                        (recursive)\n│    │    └─LeakyReLU: 3-3               [1, 100]                  --\n│    └─FCBlock: 2-3                      [1, 100]                  --\n│    │    └─Linear: 3-4                  [1, 100]                  10,100\n│    │    └─Identity: 3-5                [1, 100]                  --\n│    └─FCBlock: 2-4                      --                        (recursive)\n│    │    └─LeakyReLU: 3-6               [1, 100]                  --\n│    └─FCBlock: 2-5                      [1, 100]                  --\n│    │    └─Linear: 3-7                  [1, 100]                  10,100\n│    │    └─Identity: 3-8                [1, 100]                  --\n│    │    └─LeakyReLU: 3-9               [1, 100]                  --\n│    └─Linear: 2-6                       [1, 100]                  10,100\n==========================================================================================\nTotal params: 31,100\nTrainable params: 31,100\nNon-trainable params: 0\nTotal mult-adds (M): 0.03\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.12\nEstimated Total Size (MB): 0.13\n==========================================================================================\n\n\n\nsource\n\n\nCoefficientsFC\n\n CoefficientsFC (n_parallel:int=32, n_biquads:int=2,\n                 initial_gain_scale:float=1.0, tahn_c:float=1.0,\n                 tanh_d:float=1.0, use_zp_init:bool=False,\n                 initial_pole_mag:float=1.5, initial_zero_mag:float=0,\n                 **kwargs)\n\nA wrapper around the FC class that outputs the coefficients of a filterbank\nTest\n\nn_parallel = 32\nn_biquads = 2\nn_coeffs = 6\nn_batches = 2\ncoefficients_fc = CoefficientsFC(\n    input_size=1007,\n    hidden_sizes=[100, 100],\n    n_parallel=n_parallel,\n    n_biquads=n_biquads,\n    initial_gain_scale=1.0,\n)\n\nx = torch.randn(n_batches, 1007)\ny = coefficients_fc(x)\nassert y.shape == torch.Size([n_batches, n_parallel, n_biquads, n_coeffs])"
  },
  {
    "objectID": "modal.html",
    "href": "modal.html",
    "title": "Modal synthesis",
    "section": "",
    "text": "source\n\n\n\n frequency_to_eigenvalue (frequencies)\n\n\nsource\n\n\n\n\n eigenvalue_to_frequency (eigenvalues)\n\n\nsource\n\n\n\n\n render_modes_coeffs (eigenvalues:Union[numpy.ndarray,torch.Tensor],\n                      eigenvectors:Union[numpy.ndarray,torch.Tensor],\n                      alpha:float, beta:float,\n                      length_in_samples:int=44100, sample_rate:int=44100)\n\nRenders a batch of modes given damping coefficients.\n\nsource\n\n\n\n\n render_modes (frequency:Union[torch.Tensor,numpy.ndarray],\n               decay:Union[torch.Tensor,numpy.ndarray],\n               amplitude:Union[torch.Tensor,numpy.ndarray], initial_phase:\n               Union[torch.Tensor,numpy.ndarray,NoneType]=None,\n               length_in_samples:int=44100, sample_rate:float=44100,\n               return_sum:bool=True)\n\nRenders a batch of modes given their parameters.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfrequency\ntyping.Union[torch.Tensor, numpy.ndarray]\n\nMode frequencies\n\n\ndecay\ntyping.Union[torch.Tensor, numpy.ndarray]\n\nMode decay parameters\n\n\namplitude\ntyping.Union[torch.Tensor, numpy.ndarray]\n\nMode amplitudes\n\n\ninitial_phase\ntyping.Union[torch.Tensor, numpy.ndarray, NoneType]\nNone\n\n\n\nlength_in_samples\nint\n44100\nLength of the output signal in samples\n\n\nsample_rate\nfloat\n44100\nSample rate of the output signal\n\n\nreturn_sum\nbool\nTrue\nReturn the sum of the modes\n\n\nReturns\ntyping.Union[torch.Tensor, numpy.ndarray]\n\nRendered signal\n\n\n\n\n# vals go from 1e9 to 1e10\nvals = torch.rand(1, 64) * 1e9\nvecs = torch.rand(1, 3, 3, 64) * 0.1\nalpha = 5\nbeta = 4e-8\nsr = 16000\naudio = render_modes_coeffs(vals, vecs, alpha, beta, int(sr * 0.5), sr)\n\nfig, axs = plt.subplots(3, 1, figsize=(10, 10))\nfor i in range(3):\n    axs[i].plot(audio[0][1][i].numpy())\n    axs[i].set_title(f\"Pixel [1]{i}\")\n    ipd.display(ipd.Audio(audio[0][1][i].numpy(), rate=sr))\n\ntarget_fft = torch.fft.rfft(audio[0, 0, 0, :]).abs()\n\nfreq = torch.fft.rfftfreq(audio.shape[-1], 1/sr)\nplt.plot(freq.numpy(), target_fft.numpy())\nplt.show()\n\ndamping_coeffs = 0.5 * (alpha + beta * vals[0])\ndamped_freqs = vals[0] - damping_coeffs**2\ndamped_freqs[damped_freqs &lt; 0] = 0\ndamped_freqs = eigenvalue_to_frequency(damped_freqs)\n\nplt.stem(damped_freqs.numpy(), vecs[0, 0, 0].numpy() * 4001)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n&lt;StemContainer object of 3 artists&gt;\n\n\n\n\n\n\n\n\nGenerating a single decaying mode:\n\nfrequency = torch.tensor([440.0])\ndecay = torch.tensor([100.0])\namplitude = torch.ones(1)\nlength_in_samples = 1024\nsample_rate = 16000.0\n\nsignal = render_modes(frequency, decay, amplitude, None, length_in_samples, sample_rate)\nplt.plot(signal)"
  },
  {
    "objectID": "modal.html#rendering-utilities",
    "href": "modal.html#rendering-utilities",
    "title": "Modal synthesis",
    "section": "",
    "text": "source\n\n\n\n frequency_to_eigenvalue (frequencies)\n\n\nsource\n\n\n\n\n eigenvalue_to_frequency (eigenvalues)\n\n\nsource\n\n\n\n\n render_modes_coeffs (eigenvalues:Union[numpy.ndarray,torch.Tensor],\n                      eigenvectors:Union[numpy.ndarray,torch.Tensor],\n                      alpha:float, beta:float,\n                      length_in_samples:int=44100, sample_rate:int=44100)\n\nRenders a batch of modes given damping coefficients.\n\nsource\n\n\n\n\n render_modes (frequency:Union[torch.Tensor,numpy.ndarray],\n               decay:Union[torch.Tensor,numpy.ndarray],\n               amplitude:Union[torch.Tensor,numpy.ndarray], initial_phase:\n               Union[torch.Tensor,numpy.ndarray,NoneType]=None,\n               length_in_samples:int=44100, sample_rate:float=44100,\n               return_sum:bool=True)\n\nRenders a batch of modes given their parameters.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfrequency\ntyping.Union[torch.Tensor, numpy.ndarray]\n\nMode frequencies\n\n\ndecay\ntyping.Union[torch.Tensor, numpy.ndarray]\n\nMode decay parameters\n\n\namplitude\ntyping.Union[torch.Tensor, numpy.ndarray]\n\nMode amplitudes\n\n\ninitial_phase\ntyping.Union[torch.Tensor, numpy.ndarray, NoneType]\nNone\n\n\n\nlength_in_samples\nint\n44100\nLength of the output signal in samples\n\n\nsample_rate\nfloat\n44100\nSample rate of the output signal\n\n\nreturn_sum\nbool\nTrue\nReturn the sum of the modes\n\n\nReturns\ntyping.Union[torch.Tensor, numpy.ndarray]\n\nRendered signal\n\n\n\n\n# vals go from 1e9 to 1e10\nvals = torch.rand(1, 64) * 1e9\nvecs = torch.rand(1, 3, 3, 64) * 0.1\nalpha = 5\nbeta = 4e-8\nsr = 16000\naudio = render_modes_coeffs(vals, vecs, alpha, beta, int(sr * 0.5), sr)\n\nfig, axs = plt.subplots(3, 1, figsize=(10, 10))\nfor i in range(3):\n    axs[i].plot(audio[0][1][i].numpy())\n    axs[i].set_title(f\"Pixel [1]{i}\")\n    ipd.display(ipd.Audio(audio[0][1][i].numpy(), rate=sr))\n\ntarget_fft = torch.fft.rfft(audio[0, 0, 0, :]).abs()\n\nfreq = torch.fft.rfftfreq(audio.shape[-1], 1/sr)\nplt.plot(freq.numpy(), target_fft.numpy())\nplt.show()\n\ndamping_coeffs = 0.5 * (alpha + beta * vals[0])\ndamped_freqs = vals[0] - damping_coeffs**2\ndamped_freqs[damped_freqs &lt; 0] = 0\ndamped_freqs = eigenvalue_to_frequency(damped_freqs)\n\nplt.stem(damped_freqs.numpy(), vecs[0, 0, 0].numpy() * 4001)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n&lt;StemContainer object of 3 artists&gt;\n\n\n\n\n\n\n\n\nGenerating a single decaying mode:\n\nfrequency = torch.tensor([440.0])\ndecay = torch.tensor([100.0])\namplitude = torch.ones(1)\nlength_in_samples = 1024\nsample_rate = 16000.0\n\nsignal = render_modes(frequency, decay, amplitude, None, length_in_samples, sample_rate)\nplt.plot(signal)"
  },
  {
    "objectID": "modal.html#modal-synthesis",
    "href": "modal.html#modal-synthesis",
    "title": "Modal synthesis",
    "section": "Modal synthesis",
    "text": "Modal synthesis\n\nsource\n\ncreate_basis\n\n create_basis (mesh:skfem.mesh.mesh_2d.Mesh2D)\n\nConstruct a cell basis for the mesh.\n\nsource\n\n\ncreate_mesh\n\n create_mesh (n_refinements:int=5)\n\nInitialise a triangular mesh with n_refinements refinements.\n\nsource\n\n\nMaterial\n\n Material (rho:float, E:float, nu:float, alpha:float, beta:float)\n\nMaterial properties.\n\nsource\n\n\nMaterialRanges\n\n MaterialRanges (rho:tuple[float,float]=(500.0, 10000.0),\n                 E:tuple[float,float]=(1000000000.0, 1000000000000.0),\n                 nu:tuple[float,float]=(0.0, 0.5),\n                 alpha:tuple[float,float]=(1.0, 30.0),\n                 beta:tuple[float,float]=(1e-08, 2e-06))\n\nRanges for material properties.\n\nsource\n\n\nunscale\n\n unscale (value:float, range:tuple[float,float])\n\nUnscale a value from a given range.\n\nsource\n\n\nscale\n\n scale (value:float, range:tuple[float,float])\n\nScale a value to a given range.\n\nsource\n\n\nSystem\n\n System (material:__main__.Material,\n         mesh:Optional[skfem.mesh.mesh_2d.Mesh2D]=None,\n         basis:Optional[skfem.assembly.basis.cell_basis.CellBasis]=None,\n         k:int=128, force_cache:bool=False)\n\nDefines an enumeration across materials, with vibrational modes given as a property of the material object.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmaterial\nMaterial\n\nmaterial properties\n\n\nmesh\ntyping.Optional[skfem.mesh.mesh_2d.Mesh2D]\nNone\nmesh\n\n\nbasis\ntyping.Optional[skfem.assembly.basis.cell_basis.CellBasis]\nNone\nbasis\n\n\nk\nint\n128\nnumber of modes to compute\n\n\nforce_cache\nbool\nFalse\nforce caching at init\n\n\n\nThe modes of an object are obtained by solving the second order differential equation\n\\[\nKu + C\\dot{u} + M\\ddot{u} = f\n\\]\nwhere \\(K\\) is the stiffness matrix, \\(C\\) is the damping matrix and \\(M\\) is the damping matrix. \\(f\\) is the force we apply to the system. To solve for the natural modes of the system we set \\(f = 0\\).\nWe define a mesh using the material properties density or \\(\\rho\\), young modulus or \\(E\\), poisson number or \\(\\nu\\), and Rayleigh damping coefficients \\(\\alpha\\) and \\(\\beta\\)\nThe Lamé parameters are then given by \\[\n   \\lambda = \\frac{E}{2(1 + \\nu)}, \\quad \\mu = \\frac{E \\nu}{(1+ \\nu)(1 - 2 \\nu)},\n\\]  \nTo retrieve the vibrational modes, we must follow three steps:\n\nAssemble the stiffness and desitity matrices\nSet the boundary conditions (set degrees of freedom, by setting zeroes in the matrices)\nSolve the generalised eigenvalue problem\n\nThese steps are performed by the Material.modes property, which returns a tuple (eigenvalues, eigenvectors). This is cached for each instantiation to solve computation time if subsequent evaluations are required, e.g. in constructing a dataset.\nThe Material enumeration also provides a damped_frequencies property — also cached — which applies the formula:\n\\[\n\\omega_{i}=\\frac{1}{2 \\pi} \\sqrt{\\lambda_{i}-\\left(\\frac{\\alpha+\\beta \\lambda_{i}}{2}\\right)^{2}}\n\\]\nto derive the frequencies, where \\(\\lambda_i\\) are the eigenvalues."
  },
  {
    "objectID": "modal.html#example-usage",
    "href": "modal.html#example-usage",
    "title": "Modal synthesis",
    "section": "Example usage",
    "text": "Example usage\n\nm = MATERIALS[\"polycarbonate\"]\ns = System(m)\n\nfig, axs = plt.subplots(4, 1, figsize=(8, 8))\n\n# plot the eigenvalues\naxs[0].stem(s.eigenvalues)\naxs[0].set_title(\"Eigenvalues\")\n\n# plot the damped frequencies\naxs[1].stem(s.damped_frequencies)\naxs[1].set_title(\"Damped Frequencies\")\naxs[1].set_yscale(\"log\")\n\n# plot node gains\naxs[2].stem(np.abs(s.get_mode_gains(100)))\naxs[2].set_title(\"Mode Gains\")\n\n# plot amplitude envelopes given by damping coefficients\naxs[3].plot(\n    np.exp(-s.damping_coefficients * np.linspace(0, 2000.0 / 44100.0, 2000)[..., None])\n)\n\nfig.tight_layout()\nfig.show()\n\n/tmp/ipykernel_80158/1503038420.py:25: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n  fig.show()\n\n\n\n\n\nRendering audio from predefined materials:\n\nf_s = 44100\nT = 0.75\nnum_modes = 64\nmesh = create_mesh(n_refinements=4)\n\n# get the dimensions of the mesh\nx_min, x_max = mesh.p[0].min(), mesh.p[0].max()\ny_min, y_max = mesh.p[1].min(), mesh.p[1].max()\n\n\nprint(f\"Mesh dimensions: {x_max - x_min} x {y_max - y_min}\")\nprint(f\"Mesh bounding box: ({x_min}, {y_min}) - ({x_max}, {y_max})\")\n\nfor m in MATERIALS:\n    s = System(MATERIALS[m], k=num_modes, mesh=mesh)\n    signal = s.render(T, f_s)\n    print(f\"{m.lower()}:\")\n    save_and_display_audio(signal, f\"{m.lower()}.wav\", f_s)\n\nMesh dimensions: 1.0 x 1.0\nMesh bounding box: (0.0, 0.0) - (1.0, 1.0)\nceramic:\nglass:\nwood:\nplastic:\niron:\npolycarbonate:\nsteel:\ncustom:\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nnum_modes = 32\ncustom_ranges = MaterialRanges(\n    rho=(500, 20000),\n    E=(2e10, 5e10),\n    nu=(2, 2.11),\n    alpha=(1, 12),\n    beta=(1e-6, 2e-6),\n)\n\nMaterial.set_default_ranges(custom_ranges)\nmaterial = Material.random()\nmesh = create_mesh(n_refinements=2)\ncenter_idx = mesh.nodes_satisfying(lambda x: (x[0] == .5) & (x[1] == .5))\nprint(f\"center node: {center_idx}\")\ns = System(material, k=num_modes, mesh=mesh)\nsignal = np.clip(s.render(T, f_s, impulse_node_idx=int(center_idx)), -1, 1)\nsave_and_display_audio(signal, f\"custom.wav\", f_s)\n\ncenter node: [6]\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nRendering audio from randomly generated materials:\n\nf_s = 44100\nT = 0.75\n\nfor i in range(4):\n    m = Material.random()\n    s = System(m)\n    signal = s.render(T, f_s)\n    print(f'Material {i} — {m}:')\n    save_and_display_audio(signal, f'random_{i}.wav', f_s)\n\nMaterial 0 — Material(rho=10491.494936687797, E=23738481498.480457, nu=2.0589013931967086, alpha=11.987371289699409, beta=1.4941768366743795e-06):\nMaterial 1 — Material(rho=18149.675239893957, E=41527152275.70115, nu=2.036596716834167, alpha=5.864244986055844, beta=1.6198757753683198e-06):\nMaterial 2 — Material(rho=15295.51395426235, E=37207269181.088165, nu=2.0699158213124256, alpha=3.4064759996172103, beta=1.7399038626686502e-06):\nMaterial 3 — Material(rho=10707.976358395857, E=20442477271.769733, nu=2.0415409164909177, alpha=7.575957672808222, beta=1.1207046599371003e-06):\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "utilities.html",
    "href": "utilities.html",
    "title": "Utilities",
    "section": "",
    "text": "source\n\n\n\n plot_sample (a:numpy.ndarray, b:numpy.ndarray, gt_audio:numpy.ndarray,\n              sample_rate:int=16000, plt_ioff:bool=True)\n\nPlot a sample from the model.\n\nsource\n\n\n\n\n db20 (x:numpy.ndarray)\n\nPlot a dummy image\n\nb = np.array([[[1.0, -1.60,  0.80]]])\na = np.array([[[1.0, -0.90,  0.27]]])\n\nprint(a.shape, b.shape)\nimpulse = np.zeros(int(0.3 * 16000))\nimpulse[0] = 1.0\n\ngt_audio = np.random.randn(int(0.3 * 16000) + 1)\n\nfig = plot_sample(a, b, gt_audio)\nfig\n\n(1, 1, 3) (1, 1, 3)\n\n\n(&lt;Figure size 600x300 with 1 Axes&gt;,\n array([ 1.e+000, -7.e-001, -1.e-001, ..., -5.e-324, -5.e-324, -5.e-324]))\n\n\n\nsource\n\n\n\n\n save_and_display_audio (audio:numpy.ndarray, name:str,\n                         sample_rate:int=44100)\n\nSave audio file and display in a Jupyter notebook.\n\nsave_and_display_audio(gt_audio, \"gt_audio.wav\")\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "utilities.html#plotting",
    "href": "utilities.html#plotting",
    "title": "Utilities",
    "section": "",
    "text": "source\n\n\n\n plot_sample (a:numpy.ndarray, b:numpy.ndarray, gt_audio:numpy.ndarray,\n              sample_rate:int=16000, plt_ioff:bool=True)\n\nPlot a sample from the model.\n\nsource\n\n\n\n\n db20 (x:numpy.ndarray)\n\nPlot a dummy image\n\nb = np.array([[[1.0, -1.60,  0.80]]])\na = np.array([[[1.0, -0.90,  0.27]]])\n\nprint(a.shape, b.shape)\nimpulse = np.zeros(int(0.3 * 16000))\nimpulse[0] = 1.0\n\ngt_audio = np.random.randn(int(0.3 * 16000) + 1)\n\nfig = plot_sample(a, b, gt_audio)\nfig\n\n(1, 1, 3) (1, 1, 3)\n\n\n(&lt;Figure size 600x300 with 1 Axes&gt;,\n array([ 1.e+000, -7.e-001, -1.e-001, ..., -5.e-324, -5.e-324, -5.e-324]))\n\n\n\nsource\n\n\n\n\n save_and_display_audio (audio:numpy.ndarray, name:str,\n                         sample_rate:int=44100)\n\nSave audio file and display in a Jupyter notebook.\n\nsave_and_display_audio(gt_audio, \"gt_audio.wav\")\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "utilities.html#losses",
    "href": "utilities.html#losses",
    "title": "Utilities",
    "section": "Losses",
    "text": "Losses\n\nsource\n\nfft_loss\n\n fft_loss (pred_fft:torch.Tensor, target_fft:torch.Tensor,\n           lin_l1:float=1.0, lin_l2:float=0.0, log_l1:float=0.0,\n           log_l2:float=0.0)\n\nCompute the loss between the target and predicted fft\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npred_fft\nTensor\n\nmagnitude of the predicted fft\n\n\ntarget_fft\nTensor\n\nmagnitude of the target fft\n\n\nlin_l1\nfloat\n1.0\nweight of the linear l1 loss\n\n\nlin_l2\nfloat\n0.0\nweight of the linear l2 loss\n\n\nlog_l1\nfloat\n0.0\nweight of the log l1 loss\n\n\nlog_l2\nfloat\n0.0\nweight of the log l2 loss\n\n\nReturns\nTensor\n\nscalar loss\n\n\n\n\nsource\n\n\nFFTLoss\n\n FFTLoss (lin_l1:float=1.0, lin_l2:float=0.0, log_l1:float=0.0,\n          log_l2:float=0.0)\n\nCompute the loss between the target and predicted fft\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlin_l1\nfloat\n1.0\nweight of the linear l1 loss\n\n\nlin_l2\nfloat\n0.0\nweight of the linear l2 loss\n\n\nlog_l1\nfloat\n0.0\nweight of the log l1 loss\n\n\nlog_l2\nfloat\n0.0\nweight of the log l2 loss\n\n\n\n\nsource\n\n\nMelScaleLoss\n\n MelScaleLoss (n_mels:int=128, sample_rate:int=16000, n_fft:int=0,\n               lin_l1:float=1.0, lin_l2:float=0.0, log_l1:float=0.0,\n               log_l2:float=0.0, f_min:float=0.0,\n               f_max:Optional[float]=8000.0)\n\nCompute the loss between the target and predicted mel scale\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_mels\nint\n128\n\n\n\nsample_rate\nint\n16000\n\n\n\nn_fft\nint\n0\n\n\n\nlin_l1\nfloat\n1.0\nweight of the linear l1 loss\n\n\nlin_l2\nfloat\n0.0\nweight of the linear l2 loss\n\n\nlog_l1\nfloat\n0.0\nweight of the log l1 loss\n\n\nlog_l2\nfloat\n0.0\nweight of the log l2 loss\n\n\nf_min\nfloat\n0.0\n\n\n\nf_max\ntyping.Optional[float]\n8000.0\n\n\n\nReturns\nNone"
  },
  {
    "objectID": "utilities.html#other",
    "href": "utilities.html#other",
    "title": "Utilities",
    "section": "Other",
    "text": "Other\n\nsource\n\nto_zpk\n\n to_zpk (x:torch.Tensor, n_parallel:int, n_biquads:int,\n         gain_factor:float=1.0)\n\nConvert the parameters of the filter to zeros, poles and gain\nInput: x: (B, n_parallel, n_biquads, 5) Output: z: (B, n_parallel, n_biquads) p: (B, n_parallel, n_biquads) k: (B, n_parallel, n_biquads)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Rigid-Body Sound Synthesis with Differentiable Modal Resonators\nThis is the code for the paper Rigid-Body Sound Synthesis with Differentiable Modal Resonators. It contains the implementation of the differentiable modal resonator networks, as well as the code to train them and generate the audio samples used in the paper."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "About",
    "section": "Install",
    "text": "Install\nTo install the package, run the following command:\npip install -e '.[dev]'"
  },
  {
    "objectID": "index.html#train",
    "href": "index.html#train",
    "title": "About",
    "section": "Train",
    "text": "Train\nFor a quicker preview of the training process, please refer to this notebook.\nIn order to train the network first we need to generate the dataset. Training the network requires large dataset the generation and the training might take up to a day to complete. To do so, run the following command (adjusting the number of shapes and materials to generate):\ngenerate_dataset \\\nn_train_shapes=500 \\\nn_val_shapes=20 \\\nn_test_shapes=20 \\\nn_train_materials=500 \\\nn_val_materials=20 \\\nn_test_materials=20 \\\n++paths.data_dir=./data\nTo train the network, run the following command:\ntrain \\\n++datamodule.sample_rate=32000 \\\n++datamodule.batch_size=64 \\\n++datamodule.num_workers=8 \\\n++datamodule.train_index_map_path=./data/materials_shapes_train/index_map.csv \\\n++datamodule.val_index_map_path=./data/materials_shapes_val/index_map.csv \\\n++datamodule.test_index_map_path=./data/materials_shapes_val/index_map.csv \\\nseed=3407"
  }
]