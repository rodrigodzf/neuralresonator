# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/05_training.ipynb.

# %% auto 0
__all__ = ['MultiShapeMultiMaterialLitModule']

# %% ../nbs/05_training.ipynb 2
from typing import Any, Type

import lightning as pl
import torch
from torch import nn, optim
from torchmetrics.regression.mae import MeanAbsoluteError
from torchmetrics.regression.mse import MeanSquaredError
from torchvision.models.efficientnet import EfficientNet_B0_Weights, efficientnet_b0

from .dsp import biquad_freqz
from .utilities import plot_sample, FFTLoss
import wandb

# %% ../nbs/05_training.ipynb 3
class MultiShapeMultiMaterialLitModule(pl.LightningModule):
    def __init__(
        self,
        model: nn.Module,
        optimizer: Type[optim.Optimizer],
        scheduler: Type[optim.lr_scheduler.LRScheduler],
        criterion: nn.Module = FFTLoss(),
        **kwargs,
    ):
        super().__init__()

        self.save_hyperparameters(logger=False)

        self.model = model
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.criterion = criterion

        self.encoder = efficientnet_b0(
            weights=EfficientNet_B0_Weights.DEFAULT,
        )

        self.mse = MeanSquaredError()
        self.mae = MeanAbsoluteError()

    def forward(
        self,
        x: torch.Tensor,
    ) -> torch.Tensor:
        return self.model(x)

    def step(self, batch: Any):
        mask = batch["mask"]
        coords = batch["coords"]
        audio = batch["audio"]
        material_params = batch["material_params"]

        mag_ffts = torch.fft.rfft(
            audio.float().clamp(-1, 1),
        ).abs()

        # Repeat mask to match weights
        features = self.encoder(mask.repeat(1, 3, 1, 1).float())

        # Predict biquad coefficients
        ba = self.forward(torch.cat([features, coords, material_params], dim=-1))
        b = ba[..., :3]
        a = ba[..., 3:]

        p_mag_ffts = biquad_freqz(b, a, audio.shape[-1]).prod(dim=-2).sum(dim=-2).abs()

        loss = self.criterion(
            p_mag_ffts,
            mag_ffts,
        )

        return dict(
            loss=loss,
            a=a,
            b=b,
            p_mag_ffts=p_mag_ffts,
            mag_ffts=mag_ffts,
            audio=audio,
        )

    def get_first_and_plot(
        self,
        batch: Any,
        name: str,
    ) -> None:
        with torch.no_grad():
            # Get the first sample from the batch
            a = batch["a"][0].cpu().numpy()
            b = batch["b"][0].cpu().numpy()
            audio = batch["audio"][0].cpu().numpy()
            fig, pred_signal = plot_sample(
                a=a,
                b=b,
                gt_audio=audio,
            )
            wandb.log({f"{name}/plot": wandb.Image(fig)})
            wandb_gt_audio = wandb.Audio(audio, sample_rate=32000)
            wandb_pred_audio = wandb.Audio(pred_signal, sample_rate=32000)
            wandb.log({f"{name}/audio": [wandb_gt_audio, wandb_pred_audio]})

    def training_step(
        self,
        batch: Any,
        batch_idx: int,
    ):
        batch_output: dict = self.step(batch)
        self.log("train/loss", batch_output["loss"], on_step=True, on_epoch=False)
        if batch_idx % self.trainer.val_check_interval == 0 and self.logger:
            self.get_first_and_plot(
                batch=batch_output,
                name="train_epoch_end",
            )

        return batch_output["loss"]

    def validation_step(
        self,
        batch: Any,
        batch_idx: int,
    ):
        batch_output: dict = self.step(batch)
        self.log("val/loss", batch_output["loss"])
        return None

    def test_step(
        self,
        batch: Any,
        batch_idx: int,
    ):
        batch_output: dict = self.step(batch)

        p_mag_ffts = batch_output["p_mag_ffts"]
        mag_ffts = batch_output["mag_ffts"]

        self.mae(torch.log(p_mag_ffts + 1e-10), torch.log(mag_ffts + 1e-10))
        self.mse(torch.log(p_mag_ffts + 1e-10), torch.log(mag_ffts + 1e-10))
        return None

    def on_test_epoch_end(
        self,
    ):
        self.log("test/mae", self.mae.compute())
        self.log("test/mse", self.mse.compute())
        return None

    def configure_optimizers(
        self,
    ):
        optimizer = self.optimizer(self.parameters())
        lr_scheduler_config = {
            "scheduler": self.scheduler(optimizer),
            "monitor": "train/loss",
            "frequency": 1,
            "interval": "step",
        }

        return {
            "optimizer": optimizer,
            "lr_scheduler": lr_scheduler_config,
        }
