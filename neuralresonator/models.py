# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/04_models.ipynb.

# %% auto 0
__all__ = ['FCBlock', 'FC', 'CoefficientsFC']

# %% ../nbs/04_models.ipynb 2
import torch.nn as nn
import torch.nn.functional as F
import torch

import numpy as np
import torch
import torch.nn as nn
from typing import List
import math
from .utilities import to_zpk
from neuralresonator.dsp import (
    constrain_complex_pole_or_zero,
    pole_or_zero_to_iir_coeff,
)
from torchinfo import summary

# %% ../nbs/04_models.ipynb 3
class FCBlock(nn.Module):
    """
    A fully connected block with an activation function and optional layer normalization
    """

    def __init__(
        self,
        input_size: int,  # The size of the input
        output_size: int,  # The size of the output
        activation: nn.Module = nn.LeakyReLU(
            0.2, inplace=True
        ),  # The activation function
        layer_norm: bool = False,  # Whether to use layer normalization
    ):
        super().__init__()
        self.input_size = input_size
        self.output_size = output_size
        self.activation = activation

        self.linear = nn.Linear(input_size, output_size)
        self.ln = nn.LayerNorm(output_size) if layer_norm else nn.Identity()

    def forward(
        self,
        x: torch.Tensor,
    ) -> torch.Tensor:
        return self.activation(self.ln(self.linear(x)))


class FC(nn.Module):
    """
    A fully connected network with an activation function and optional layer normalization
    """

    def __init__(
        self,
        input_size: int,  # The size of the input
        output_size: int,  # The size of the output
        hidden_sizes: List[int],  # The sizes of the hidden layers
        activation: nn.Module = nn.LeakyReLU(
            0.2, inplace=True
        ),  # The activation function
        layer_norm: bool = False,  # Whether to use layer normalization
    ):
        super().__init__()
        self.input_size = input_size
        self.output_size = output_size
        self.hidden_sizes = hidden_sizes
        self.activation = activation

        layers = []

        # First layer
        layers.append(
            FCBlock(
                input_size,
                hidden_sizes[0],
                activation=activation,
                layer_norm=layer_norm,
            )
        )

        # Hidden layers
        for i in range(len(hidden_sizes)):
            # Set the input size equal to the output size
            n = hidden_sizes[i]

            layers.append(
                FCBlock(
                    hidden_sizes[i],
                    hidden_sizes[i],
                    activation=activation,
                    layer_norm=layer_norm,
                )
            )

        # Output layer
        layers.append(nn.Linear(hidden_sizes[-1], output_size))

        self.network = nn.Sequential(*layers)

    def forward(
        self,
        x: torch.Tensor,
    ) -> torch.Tensor:
        return self.network(x)

# %% ../nbs/04_models.ipynb 6
class CoefficientsFC(nn.Module):
    """
    A wrapper around the FC class that outputs the coefficients of a filterbank
    """

    def __init__(
        self,
        n_parallel: int = 32,
        n_biquads: int = 2,
        initial_gain_scale: float = 1.0,
        tahn_c: float = 1.0,
        tanh_d: float = 1.0,
        use_zp_init: bool = False,
        initial_pole_mag: float = 1.5,
        initial_zero_mag: float = 0,
        **kwargs,
    ):
        super().__init__()
        self.n_parallel = n_parallel
        self.n_biquads = n_biquads
        self.initial_gain_scale = initial_gain_scale
        self.tahn_c = tahn_c
        self.tanh_d = tanh_d
        self.use_zp_init = use_zp_init

        if use_zp_init:
            z_offset = torch.polar(
                torch.zeros(self.n_parallel, self.n_biquads) * initial_zero_mag,
                torch.rand(self.n_parallel, self.n_biquads) * np.pi,
            )
            p_offset = torch.polar(
                torch.ones(self.n_parallel, self.n_biquads) * initial_pole_mag,
                torch.rand(self.n_parallel, self.n_biquads) * np.pi,
            )
            self.register_buffer("z_offset", z_offset)
            self.register_buffer("p_offset", p_offset)

        # override the output size
        kwargs["output_size"] = n_parallel * n_biquads * 5
        self.fc = FC(**kwargs)

    def forward(
        self,
        x: torch.Tensor,  # A concatenated tensor of features, coodinates, and material properties
    ) -> torch.Tensor:  # The concatenated coefficients of the filterbank
        x = self.fc(x)

        z, p, k = to_zpk(
            x=x,
            n_parallel=self.n_parallel,
            n_biquads=self.n_biquads,
            gain_factor=self.initial_gain_scale,
        )

        if self.use_zp_init:
            z = self.z_offset + z
            p = self.p_offset + p

        b = pole_or_zero_to_iir_coeff(z) * k[..., None]
        a = pole_or_zero_to_iir_coeff(
            constrain_complex_pole_or_zero(
                p,
                c=self.tahn_c,
                d=self.tanh_d,
            )
        )

        return torch.concat([b, a], dim=-1)
