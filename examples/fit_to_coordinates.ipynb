{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting to a single material shape combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "import wandb\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from skfem import MeshTri\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from neuralresonator.data import SingleShapeDataset\n",
    "from neuralresonator.dsp import (biquad_freqz, constrain_complex_pole_or_zero,\n",
    "                                 pole_or_zero_to_iir_coeff)\n",
    "from neuralresonator.models import CoefficientsFC\n",
    "from neuralresonator.utilities import (FFTLoss, MelScaleLoss, plot_sample,\n",
    "                                       to_zpk)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a material shape combination and fit it to a set of coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 16000\n",
    "audio_length_in_seconds = 0.2\n",
    "samples = int(sample_rate * audio_length_in_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FitSingleShape(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            output_folder: Path,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = CoefficientsFC(\n",
    "            input_size=2,\n",
    "            hidden_sizes=[1024] * 2,\n",
    "            layer_norm=False,\n",
    "        )\n",
    "\n",
    "        self.output_folder = output_folder\n",
    "\n",
    "        self.criterion = FFTLoss(\n",
    "            lin_l1=1.0,\n",
    "            lin_l2=0.0,\n",
    "            log_l1=0.01,\n",
    "            log_l2=0.0,\n",
    "        )\n",
    "\n",
    "        self.training_outputs = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        coords = batch[\"coords\"]\n",
    "        audio = batch[\"audio\"]\n",
    "\n",
    "        mag_ffts = torch.fft.rfft(\n",
    "            audio.float().clamp(-1, 1),\n",
    "        ).abs()\n",
    "\n",
    "        ba = self.forward(coords)\n",
    "        b = ba[..., :3]\n",
    "        a = ba[..., 3:]\n",
    "\n",
    "        p_mag_ffts = (\n",
    "            biquad_freqz(b, a, audio.shape[-1]).prod(dim=-2).sum(dim=-2).abs()\n",
    "        )\n",
    "\n",
    "        loss = self.criterion(\n",
    "            p_mag_ffts,\n",
    "            mag_ffts,\n",
    "        )\n",
    "\n",
    "        self.log(\"train/loss\", loss)\n",
    "\n",
    "        output = dict(\n",
    "            loss=loss,\n",
    "            a=a,\n",
    "            b=b,\n",
    "            audio=audio,\n",
    "        )\n",
    "\n",
    "        self.training_outputs.append(\n",
    "            output,\n",
    "        )\n",
    "\n",
    "        return output[\"loss\"]\n",
    "    \n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        batch = self.training_outputs[-1]\n",
    "        self.log(\"train/loss\", batch[\"loss\"], prog_bar=True,)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            audio = batch[\"audio\"][0].cpu().numpy()\n",
    "            fig, pred_signal = plot_sample(\n",
    "                a=batch[\"a\"][0].cpu().numpy(),\n",
    "                b=batch[\"b\"][0].cpu().numpy(),\n",
    "                gt_audio=audio,\n",
    "            )\n",
    "            self.logger.experiment.log({f\"train_epoch_end\": wandb.Image(fig)})\n",
    "            wandb_gt_audio = wandb.Audio(audio, sample_rate=sample_rate)\n",
    "            wandb_pred_audio = wandb.Audio(pred_signal, sample_rate=sample_rate)\n",
    "            self.logger.experiment.log(\n",
    "                {f\"train_epoch_end/audio\": [wandb_gt_audio, wandb_pred_audio]}\n",
    "            )\n",
    "\n",
    "        self.training_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(lr=5e-5, params=self.parameters())\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            factor=0.8,\n",
    "            patience=300,\n",
    "        )\n",
    "\n",
    "        lr_scheduler_config = {\n",
    "            \"scheduler\": scheduler,\n",
    "            \"monitor\": \"train/loss\",\n",
    "            \"frequency\": 1,\n",
    "            \"interval\": \"step\",\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": lr_scheduler_config,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = Path(\"output\")\n",
    "if not output_folder.exists():\n",
    "    output_folder.mkdir()\n",
    "\n",
    "model = FitSingleShape(\n",
    "    output_folder=output_folder,\n",
    ")\n",
    "\n",
    "dataset = SingleShapeDataset(\n",
    "    mesh=MeshTri.init_circle(smoothed=True),\n",
    "    audio_length_in_seconds=audio_length_in_seconds,\n",
    "    sample_rate=sample_rate,\n",
    "    n_refinements=3,\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = WandbLogger(\n",
    "    project=\"neuralresonator\",\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=200,\n",
    "    logger=logger,\n",
    "    callbacks=[\n",
    "        lr_monitor,\n",
    "    ],\n",
    "    enable_checkpointing=False,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model=model,\n",
    "    train_dataloaders=train_loader,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
