{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit to arbitrary shapes\n",
    "\n",
    "This notebook shows how to overfit a neural resonator to arbitrary shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "import wandb\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from torchvision.models.efficientnet import (EfficientNet_B0_Weights,\n",
    "                                             efficientnet_b0)\n",
    "\n",
    "from neuralresonator.data import (MultiShapeMultiMaterialDataModule,\n",
    "                                  generate_random_dataset)\n",
    "from neuralresonator.dsp import biquad_freqz\n",
    "from neuralresonator.modal import MATERIALS\n",
    "from neuralresonator.models import CoefficientsFC\n",
    "from neuralresonator.utilities import FFTLoss, MelScaleLoss, plot_sample\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a random dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shapes = 5\n",
    "n_materials = 1\n",
    "n_refinements = 3\n",
    "n_vertices = 13\n",
    "sample_rate = 16000\n",
    "audio_length_in_seconds = 0.2\n",
    "samples = int(sample_rate * audio_length_in_seconds)\n",
    "\n",
    "data_dir = Path(\"data\")\n",
    "if not data_dir.exists():\n",
    "    data_dir.mkdir()\n",
    "\n",
    "pl.seed_everything(3407, workers=True)\n",
    "\n",
    "generate_random_dataset(\n",
    "    n_shapes=n_shapes,\n",
    "    n_materials=n_materials,\n",
    "    materials=[MATERIALS['polycarbonate']],\n",
    "    n_vertices=n_vertices,\n",
    "    n_modes=32,\n",
    "    n_refinements=n_refinements,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lightning module class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FitShapes(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_folder: Path,\n",
    "        lr: float = 1e-4,\n",
    "        n_parallel: int = 32,\n",
    "        n_biquads: int = 2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_folder = output_folder\n",
    "        self.n_parallel = n_parallel\n",
    "        self.n_biquads = n_biquads\n",
    "\n",
    "        self.encoder = efficientnet_b0(\n",
    "            weights=EfficientNet_B0_Weights.DEFAULT,\n",
    "        )\n",
    "\n",
    "        self.lr = lr\n",
    "\n",
    "        self.model = CoefficientsFC(\n",
    "            input_size=3,\n",
    "            hidden_sizes=[1024] * 2,\n",
    "            layer_norm=False,\n",
    "        )\n",
    "\n",
    "        self.criterion = FFTLoss(\n",
    "            lin_l1=1.0,\n",
    "            lin_l2=0.0,\n",
    "            log_l1=0.2,\n",
    "            log_l2=0.0,\n",
    "        )\n",
    "\n",
    "        self.training_outputs = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        mask = batch[\"mask\"]\n",
    "        coords = batch[\"coords\"]\n",
    "        audio = batch[\"audio\"]\n",
    "\n",
    "        mag_ffts = torch.fft.rfft(\n",
    "            audio.float().clamp(-1, 1),\n",
    "        ).abs()\n",
    "\n",
    "        features = self.encoder(mask.repeat(1, 3, 1, 1).float())\n",
    "        \n",
    "        # Predict biquad coefficients\n",
    "        # using the mean of the features makes the model overfit faster to the shapes\n",
    "        ba = self.forward(torch.cat([features.mean(-1, keepdim=True), coords], dim=-1))\n",
    "        b = ba[..., :3]\n",
    "        a = ba[..., 3:]\n",
    "\n",
    "        p_mag_ffts = biquad_freqz(b, a, audio.shape[-1]).prod(dim=-2).sum(dim=-2).abs()\n",
    "\n",
    "        loss = self.criterion(\n",
    "            p_mag_ffts,\n",
    "            mag_ffts,\n",
    "        )\n",
    "\n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=False)\n",
    "\n",
    "        output = dict(\n",
    "            loss=loss,\n",
    "            a=a,\n",
    "            b=b,\n",
    "            audio=audio,\n",
    "        )\n",
    "\n",
    "        self.training_outputs.append(\n",
    "            output,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        batch = self.training_outputs[-1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            audio = batch[\"audio\"][0].cpu().numpy()\n",
    "            fig, pred_signal = plot_sample(\n",
    "                a=batch[\"a\"][0].cpu().numpy(),\n",
    "                b=batch[\"b\"][0].cpu().numpy(),\n",
    "                gt_audio=audio,\n",
    "            )\n",
    "            self.logger.experiment.log({f\"train_epoch_end\": wandb.Image(fig)})\n",
    "            wandb_gt_audio = wandb.Audio(audio, sample_rate=sample_rate)\n",
    "            wandb_pred_audio = wandb.Audio(pred_signal, sample_rate=sample_rate)\n",
    "            self.logger.experiment.log(\n",
    "                {f\"train_epoch_end/audio\": [wandb_gt_audio, wandb_pred_audio]}\n",
    "            )\n",
    "\n",
    "        self.training_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(lr=self.lr, params=self.parameters())\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            factor=0.8,\n",
    "            patience=600,\n",
    "        )\n",
    "\n",
    "        lr_scheduler_config = {\n",
    "            \"scheduler\": scheduler,\n",
    "            \"monitor\": \"train/loss\",\n",
    "            \"frequency\": 1,\n",
    "            \"interval\": \"step\",\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": lr_scheduler_config,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = Path(\"output\")\n",
    "if not output_folder.exists():\n",
    "    output_folder.mkdir()\n",
    "\n",
    "datamodule = MultiShapeMultiMaterialDataModule(\n",
    "    train_index_map_path=data_dir / \"index_map.csv\",\n",
    "    val_index_map_path=data_dir / \"index_map.csv\",\n",
    "    test_index_map_path=data_dir / \"index_map.csv\",\n",
    "    batch_size=16,\n",
    "    num_workers=8,\n",
    "    audio_length_in_seconds=audio_length_in_seconds,\n",
    ")\n",
    "\n",
    "model = FitShapes(\n",
    "    output_folder=output_folder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = WandbLogger(\n",
    "    project=\"neuralresonator\",\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=300,\n",
    "    logger=logger,\n",
    "    callbacks=[lr_monitor],\n",
    ")\n",
    "trainer.fit(\n",
    "    model=model,\n",
    "    datamodule=datamodule,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
